"""
This type stub file was generated by pyright.
"""

from torch import nn

"""Multi-Head Attention layer definition."""
class ESPNETMultiHeadedAttention(nn.Module):
    """Multi-Head Attention layer.
    Args:
        n_head: The number of heads.
        n_feat: The number of features.
        dropout: Dropout rate.
    """
    def __init__(self, n_feat, n_head, dropout) -> None:
        """Construct an MultiHeadedAttention object."""
        ...
    
    def forward_qkv(self, query, key, value, **kwargs): # -> tuple[Any, Any, Any]:
        """Transform query, key and value.
        Args:
            query: Query tensor  B X T1 X C
            key: Key tensor B X T2 X C
            value: Value tensor  B X T2 X C
        Returns:
            torch.Tensor: Transformed query tensor  B X n_head X T1 X d_k
            torch.Tensor: Transformed key tensor B X n_head X T2 X d_k
            torch.Tensor: Transformed value tensor  B X n_head X T2 X d_k
        """
        ...
    
    def forward_attention(self, value, scores, mask): # -> Any:
        """Compute attention context vector.
        Args:
            value: Transformed value B X n_head X T2 X d_k.
            scores: Attention score  B X n_head X T1 X T2
            mask: Mask  T2 X B
        Returns:
            torch.Tensor: Transformed value  B X T1 X d_model
                weighted by the attention score  B X T1 X T2
        """
        ...
    
    def forward(self, query, key, value, key_padding_mask=..., **kwargs): # -> tuple[Any, None]:
        """Compute scaled dot product attention.
        Args:
            query (torch.Tensor): Query tensor T X B X C
            key (torch.Tensor): Key tensor T X B X C
            value (torch.Tensor): Value tensor T X B X C
            mask (torch.Tensor): Mask tensor T X B
        Returns:
            torch.Tensor: Output tensor T X B X D.
        """
        ...
    


class RelPositionMultiHeadedAttention(ESPNETMultiHeadedAttention):
    """Multi-Head Attention layer with relative position encoding.
    Paper: https://arxiv.org/abs/1901.02860
    Args:
        n_head: The number of heads.
        n_feat: The number of features.
        dropout: Dropout rate.
        zero_triu: Whether to zero the upper triangular part of attention matrix.
    """
    def __init__(self, n_feat, n_head, dropout, zero_triu=...) -> None:
        """Construct an RelPositionMultiHeadedAttention object."""
        ...
    
    def rel_shift(self, x): # -> Tensor:
        """Compute relative positional encoding.
        Args:
            x: Input tensor B X n_head X T X 2T-1
        Returns:
            torch.Tensor: Output tensor.
        """
        ...
    
    def forward(self, query, key, value, pos_emb, key_padding_mask=..., **kwargs): # -> tuple[Any, None]:
        """Compute scaled dot product attention.
        Args:
            query: Query tensor T X B X C
            key: Key tensor T X B X C
            value: Value tensor T X B X C
            pos_emb: Positional embedding tensor B X 2T-1 X C
            key_padding_mask: Mask tensor T X B
        Returns:
            torch.Tensor: Output tensor T X B X C.
        """
        ...
    


class RotaryPositionMultiHeadedAttention(ESPNETMultiHeadedAttention):
    def __init__(self, n_feat, n_head, dropout, precision, rotary_emd_base=...) -> None:
        """Construct an RotaryPositionMultiHeadedAttention object."""
        ...
    
    def forward(self, query, key, value, key_padding_mask=..., **kwargs): # -> tuple[Any, None]:
        """Compute rotary position attention.
        Args:
            query: Query tensor T X B X C
            key: Key tensor T X B X C
            value: Value tensor T X B X C
            key_padding_mask: Mask tensor T X B
        Returns:
            torch.Tensor: Output tensor T X B X D.
        Notes:
            Assumes self attn
        """
        ...
    


