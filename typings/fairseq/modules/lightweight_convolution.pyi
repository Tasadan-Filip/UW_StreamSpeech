"""
This type stub file was generated by pyright.
"""

import torch.nn as nn
from fairseq.incremental_decoding_utils import with_incremental_state

def LightweightConv(input_size, kernel_size=..., padding_l=..., num_heads=..., weight_dropout=..., weight_softmax=..., bias=...): # -> LightconvLayer | LightweightConv1dTBC:
    ...

class LightweightConv1d(nn.Module):
    """Lightweight Convolution assuming the input is BxCxT
    This is just an example that explains LightConv clearer than the TBC version.
    We don't use this module in the model.

    Args:
        input_size: # of channels of the input and output
        kernel_size: convolution channels
        padding: padding
        num_heads: number of heads used. The weight is of shape
            `(num_heads, 1, kernel_size)`
        weight_softmax: normalize the weight with softmax before the convolution

    Shape:
        Input: BxCxT, i.e. (batch_size, input_size, timesteps)
        Output: BxCxT, i.e. (batch_size, input_size, timesteps)

    Attributes:
        weight: the learnable weights of the module of shape
            `(num_heads, 1, kernel_size)`
        bias: the learnable bias of the module of shape `(input_size)`
    """
    def __init__(self, input_size, kernel_size=..., padding=..., num_heads=..., weight_softmax=..., bias=..., weight_dropout=...) -> None:
        ...
    
    def reset_parameters(self): # -> None:
        ...
    
    def forward(self, input): # -> Tensor:
        """
        input size: B x C x T
        output size: B x C x T
        """
        ...
    


@with_incremental_state
class LightweightConv1dTBC(nn.Module):
    """Lightweight Convolution assuming the input is TxBxC
    Args:
        input_size: # of channels of the input
        kernel_size: convolution channels
        padding_l: padding to the left when using "same" padding
        num_heads: number of heads used. The weight is of shape (num_heads, 1, kernel_size)
        weight_dropout: the drop rate of the DropConnect to drop the weight
        weight_softmax: normalize the weight with softmax before the convolution
        bias: use bias

    Shape:
        Input: TxBxC, i.e. (timesteps, batch_size, input_size)
        Output: TxBxC, i.e. (timesteps, batch_size, input_size)

    Attributes:
        weight: the learnable weights of the module of shape
            `(num_heads, 1, kernel_size)`
        bias:   the learnable bias of the module of shape `(input_size)`
    """
    def __init__(self, input_size, kernel_size=..., padding_l=..., num_heads=..., weight_dropout=..., weight_softmax=..., bias=...) -> None:
        ...
    
    def reset_parameters(self): # -> None:
        ...
    
    def forward(self, x, incremental_state=..., unfold=...): # -> Tensor:
        """Assuming the input, x, of the shape T x B x C and producing an output in the shape T x B x C
        args:
            x: Input of shape T x B x C, i.e. (timesteps, batch_size, input_size)
            incremental_state: A dict to keep the state
            unfold: unfold the input or not. If not, we use the matrix trick instead
        """
        ...
    
    def prepare_for_onnx_export_(self): # -> None:
        ...
    
    def reorder_incremental_state(self, incremental_state, new_order): # -> None:
        ...
    
    def extra_repr(self): # -> str:
        ...
    


