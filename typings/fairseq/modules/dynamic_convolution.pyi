"""
This type stub file was generated by pyright.
"""

import torch.nn as nn
from typing import Dict, Optional
from fairseq.incremental_decoding_utils import FairseqIncrementalState, with_incremental_state
from torch import Tensor

def DynamicConv(input_size, kernel_size=..., padding_l=..., num_heads=..., weight_dropout=..., weight_softmax=..., renorm_padding=..., bias=..., conv_bias=..., query_size=..., in_proj=...): # -> DynamicconvLayer | DynamicConv1dTBC:
    ...

def Linear(in_features, out_features, bias=...): # -> Linear:
    ...

@with_incremental_state
class DynamicConv1dTBC(nn.Module):
    """Dynamic lightweight convolution taking T x B x C inputs
    Args:
        input_size: # of channels of the input
        kernel_size: convolution channels
        padding_l: padding to the left when using "same" padding
        num_heads: number of heads used. The weight is of shape (num_heads, 1, kernel_size)
        weight_dropout: the drop rate of the DropConnect to drop the weight
        weight_softmax: normalize the weight with softmax before the convolution
        renorm_padding: re-normalize the filters to ignore the padded part (only the non-padding parts sum up to 1)
        bias: use bias
        conv_bias: bias of the convolution
        query_size: specified when feeding a different input as the query
        in_proj: project the input and generate the filter together

    Shape:
        Input: TxBxC, i.e. (timesteps, batch_size, input_size)
        Output: TxBxC, i.e. (timesteps, batch_size, input_size)

    Attributes:
        weight: the learnable weights of the module of shape
            `(num_heads, 1, kernel_size)`
        bias:   the learnable bias of the module of shape `(input_size)`
    """
    def __init__(self, input_size, kernel_size=..., padding_l=..., num_heads=..., weight_dropout=..., weight_softmax=..., renorm_padding=..., bias=..., conv_bias=..., query_size=..., in_proj=...) -> None:
        ...
    
    @property
    def in_proj(self):
        ...
    
    def reset_parameters(self): # -> None:
        ...
    
    def forward(self, x, incremental_state=..., query=..., unfold=...): # -> Tensor:
        """Assuming the input, x, of the shape T x B x C and producing an output in the shape T x B x C
        args:
            x: Input of shape T x B x C, i.e. (timesteps, batch_size, input_size)
            incremental_state: A dict to keep the state
            unfold: unfold the input or not. If not, we use the matrix trick instead
            query: use the specified query to predict the conv filters
        """
        ...
    
    def reorder_incremental_state(self, incremental_state, new_order): # -> None:
        ...
    
    def extra_repr(self): # -> str:
        ...
    


class DynamicConv_scripatable(nn.Module, FairseqIncrementalState):
    """Dynamic lightweight convolution taking T x B x C inputs
    Args:
        input_size: # of channels of the input
        kernel_size: convolution channels
        padding_l: padding to the left when using "same" padding
        num_heads: number of heads used. The weight is of shape (num_heads, 1, kernel_size)
        weight_dropout: the drop rate of the DropConnect to drop the weight
        weight_softmax: normalize the weight with softmax before the convolution
        renorm_padding: re-normalize the filters to ignore the padded part (only the non-padding parts sum up to 1)
        bias: use bias
        conv_bias: bias of the convolution
        query_size: specified when feeding a different input as the query
        in_proj: project the input and generate the filter together

    Shape:
        Input: TxBxC, i.e. (timesteps, batch_size, input_size)
        Output: TxBxC, i.e. (timesteps, batch_size, input_size)

    Attributes:
        weight: the learnable weights of the module of shape
            `(num_heads, 1, kernel_size)`
        bias:   the learnable bias of the module of shape `(input_size)`
    """
    def __init__(self, input_size, kernel_size=..., padding_l=..., num_heads=..., weight_dropout=..., weight_softmax=..., renorm_padding=..., bias=..., conv_bias=..., query_size=..., in_proj=...) -> None:
        ...
    
    def reset_parameters(self): # -> None:
        ...
    
    def forward(self, x, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = ..., query: Optional[Tensor] = ...): # -> Tensor:
        """Assuming the input, x, of the shape T x B x C and producing an output in the shape T x B x C
        args:
            x: Input of shape T x B x C, i.e. (timesteps, batch_size, input_size)
            incremental_state: A dict to keep the state
            unfold: unfold the input or not. If not, we use the matrix trick instead
            query: use the specified query to predict the conv filters
        """
        ...
    
    def reorder_incremental_state(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]], new_order: Tensor): # -> None:
        ...
    
    def extra_repr(self): # -> str:
        ...
    


