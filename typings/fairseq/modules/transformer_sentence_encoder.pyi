"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from typing import Optional, Tuple

def init_bert_params(module): # -> None:
    """
    Initialize the weights specific to the BERT Model.
    This overrides the default initializations depending on the specified arguments.
        1. If normal_init_linear_weights is set then weights of linear
           layer will be initialized using the normal distribution and
           bais will be set to the specified value.
        2. If normal_init_embed_weights is set then weights of embedding
           layer will be initialized using the normal distribution.
        3. If normal_init_proj_weights is set then weights of
           in_project_weight for MultiHeadAttention initialized using
           the normal distribution (to be validated).
    """
    ...

class TransformerSentenceEncoder(nn.Module):
    """
    Implementation for a Bi-directional Transformer based Sentence Encoder used
    in BERT/XLM style pre-trained models.

    This first computes the token embedding using the token embedding matrix,
    position embeddings (if specified) and segment embeddings
    (if specified). After applying the specified number of
    TransformerEncoderLayers, it outputs all the internal states of the
    encoder as well as the final representation associated with the first
    token (usually CLS token).

    Input:
        - tokens: B x T matrix representing sentences
        - segment_labels: B x T matrix representing segment label for tokens

    Output:
        - a tuple of the following:
            - a list of internal model states used to compute the
              predictions where each tensor has shape T x B x C
            - sentence representation associated with first input token
              in format B x C.
    """
    def __init__(self, padding_idx: int, vocab_size: int, num_encoder_layers: int = ..., embedding_dim: int = ..., ffn_embedding_dim: int = ..., num_attention_heads: int = ..., dropout: float = ..., attention_dropout: float = ..., activation_dropout: float = ..., layerdrop: float = ..., max_seq_len: int = ..., num_segments: int = ..., use_position_embeddings: bool = ..., offset_positions_by_padding: bool = ..., encoder_normalize_before: bool = ..., apply_bert_init: bool = ..., activation_fn: str = ..., learned_pos_embedding: bool = ..., embed_scale: float = ..., freeze_embeddings: bool = ..., n_trans_layers_to_freeze: int = ..., export: bool = ..., traceable: bool = ..., q_noise: float = ..., qn_block_size: int = ...) -> None:
        ...
    
    def build_embedding(self, vocab_size, embedding_dim, padding_idx): # -> Embedding:
        ...
    
    def build_transformer_sentence_encoder_layer(self, embedding_dim, ffn_embedding_dim, num_attention_heads, dropout, attention_dropout, activation_dropout, activation_fn, export, q_noise, qn_block_size): # -> TransformerSentenceEncoderLayer:
        ...
    
    def forward(self, tokens: torch.Tensor, segment_labels: torch.Tensor = ..., last_state_only: bool = ..., positions: Optional[torch.Tensor] = ..., token_embeddings: Optional[torch.Tensor] = ..., attn_mask: Optional[torch.Tensor] = ...) -> Tuple[torch.Tensor, torch.Tensor]:
        ...
    


