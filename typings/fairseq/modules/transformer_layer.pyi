"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from typing import Dict, List, Optional
from torch import Tensor

class TransformerEncoderLayerBase(nn.Module):
    """Encoder layer block.

    In the original paper each operation (multi-head attention or FFN) is
    postprocessed with: `dropout -> add residual -> layernorm`. In the
    tensor2tensor code they suggest that learning is more robust when
    preprocessing each layer with layernorm and postprocessing with:
    `dropout -> add residual`. We default to the approach in the paper, but the
    tensor2tensor approach can be enabled by setting
    *cfg.encoder.normalize_before* to ``True``.

    Args:
        args (argparse.Namespace): parsed command-line arguments
    """
    def __init__(self, cfg, return_fc=...) -> None:
        ...
    
    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size): # -> Linear:
        ...
    
    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size): # -> Linear:
        ...
    
    def build_self_attention(self, embed_dim, cfg): # -> MultiheadAttention:
        ...
    
    def residual_connection(self, x, residual):
        ...
    
    def upgrade_state_dict_named(self, state_dict, name): # -> None:
        """
        Rename layer norm states from `...layer_norms.0.weight` to
        `...self_attn_layer_norm.weight` and `...layer_norms.1.weight` to
        `...final_layer_norm.weight`
        """
        ...
    
    def forward(self, x, encoder_padding_mask: Optional[Tensor], attn_mask: Optional[Tensor] = ...): # -> Tensor | tuple[Any, Any] | Any:
        """
        Args:
            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`
            encoder_padding_mask (ByteTensor): binary ByteTensor of shape
                `(batch, seq_len)` where padding elements are indicated by ``1``.
            attn_mask (ByteTensor): binary tensor of shape `(tgt_len, src_len)`,
                where `tgt_len` is the length of output and `src_len` is the
                length of input, though here both are equal to `seq_len`.
                `attn_mask[tgt_i, src_j] = 1` means that when calculating the
                embedding for `tgt_i`, we exclude (mask out) `src_j`. This is
                useful for strided self-attention.

        Returns:
            encoded output of shape `(seq_len, batch, embed_dim)`
        """
        ...
    


class TransformerEncoderLayer(TransformerEncoderLayerBase):
    def __init__(self, args) -> None:
        ...
    
    def build_self_attention(self, embed_dim, args): # -> MultiheadAttention:
        ...
    


class TransformerDecoderLayerBase(nn.Module):
    """Decoder layer block.

    In the original paper each operation (multi-head attention, encoder
    attention or FFN) is postprocessed with: `dropout -> add residual ->
    layernorm`. In the tensor2tensor code they suggest that learning is more
    robust when preprocessing each layer with layernorm and postprocessing with:
    `dropout -> add residual`. We default to the approach in the paper, but the
    tensor2tensor approach can be enabled by setting
    *cfg.decoder.normalize_before* to ``True``.

    Args:
        args (argparse.Namespace): parsed command-line arguments
        no_encoder_attn (bool, optional): whether to attend to encoder outputs
            (default: False).
    """
    def __init__(self, cfg, no_encoder_attn=..., add_bias_kv=..., add_zero_attn=...) -> None:
        ...
    
    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size): # -> Linear:
        ...
    
    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size): # -> Linear:
        ...
    
    def build_self_attention(self, embed_dim, cfg, add_bias_kv=..., add_zero_attn=...): # -> MultiheadAttention:
        ...
    
    def build_encoder_attention(self, embed_dim, cfg): # -> MultiheadAttention:
        ...
    
    def prepare_for_onnx_export_(self): # -> None:
        ...
    
    def residual_connection(self, x, residual):
        ...
    
    def forward(self, x, encoder_out: Optional[torch.Tensor] = ..., encoder_padding_mask: Optional[torch.Tensor] = ..., incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = ..., prev_self_attn_state: Optional[List[torch.Tensor]] = ..., prev_attn_state: Optional[List[torch.Tensor]] = ..., self_attn_mask: Optional[torch.Tensor] = ..., self_attn_padding_mask: Optional[torch.Tensor] = ..., need_attn: bool = ..., need_head_weights: bool = ...): # -> tuple[Any, Any, list[Tensor | None]] | tuple[Any, Any, None]:
        """
        Args:
            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`
            encoder_padding_mask (ByteTensor, optional): binary
                ByteTensor of shape `(batch, src_len)` where padding
                elements are indicated by ``1``.
            need_attn (bool, optional): return attention weights
            need_head_weights (bool, optional): return attention weights
                for each head (default: return average over heads).

        Returns:
            encoded output of shape `(seq_len, batch, embed_dim)`
        """
        ...
    
    def make_generation_fast_(self, need_attn: bool = ..., **kwargs): # -> None:
        ...
    


class TransformerDecoderLayer(TransformerDecoderLayerBase):
    def __init__(self, args, no_encoder_attn=..., add_bias_kv=..., add_zero_attn=...) -> None:
        ...
    
    def build_self_attention(self, embed_dim, args, add_bias_kv=..., add_zero_attn=...): # -> MultiheadAttention:
        ...
    
    def build_encoder_attention(self, embed_dim, args): # -> MultiheadAttention:
        ...
    


