"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from typing import Callable, Optional

class TransformerSentenceEncoderLayer(nn.Module):
    """
    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained
    models.
    """
    def __init__(self, embedding_dim: int = ..., ffn_embedding_dim: int = ..., num_attention_heads: int = ..., dropout: float = ..., attention_dropout: float = ..., activation_dropout: float = ..., activation_fn: str = ..., export: bool = ..., q_noise: float = ..., qn_block_size: int = ..., init_fn: Callable = ...) -> None:
        ...
    
    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size): # -> Linear:
        ...
    
    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size): # -> Linear:
        ...
    
    def build_self_attention(self, embed_dim, num_attention_heads, dropout, self_attention, q_noise, qn_block_size): # -> MultiheadAttention:
        ...
    
    def forward(self, x: torch.Tensor, self_attn_mask: Optional[torch.Tensor] = ..., self_attn_padding_mask: Optional[torch.Tensor] = ...): # -> tuple[Any | Tensor, Any]:
        """
        LayerNorm is applied either before or after the self-attention/ffn
        modules similar to the original Transformer implementation.
        """
        ...
    


