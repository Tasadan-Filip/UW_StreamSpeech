"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from fairseq.dataclass import FairseqDataclass

"""
Used for EMA tracking a given pytorch module. The user is responsible for calling step()
and setting the appropriate decay
"""
@dataclass
class EMAModuleConfig(FairseqDataclass):
    ema_decay: float = ...
    ema_fp32: bool = ...


class EMAModule:
    """Exponential Moving Average of Fairseq Models"""
    def __init__(self, model, config: EMAModuleConfig, device=..., skip_keys=...) -> None:
        """
        @param model model to initialize the EMA with
        @param config EMAConfig object with configuration like
        ema_decay, ema_update_freq, ema_fp32
        @param device If provided, copy EMA to this device (e.g. gpu).
        Otherwise EMA is in the same device as the model.
        """
        ...
    
    def build_fp32_params(self, state_dict=...): # -> None:
        """
        Store a copy of the EMA params in fp32.
        If state dict is passed, the EMA params is copied from
        the provided state dict. Otherwise, it is copied from the
        current EMA model parameters.
        """
        ...
    
    def restore(self, state_dict, build_fp32_params=...): # -> None:
        """Load data from a model spec into EMA model"""
        ...
    
    def set_decay(self, decay): # -> None:
        ...
    
    def get_decay(self): # -> float:
        ...
    
    @torch.no_grad()
    def step(self, new_model): # -> None:
        ...
    
    def reverse(self, model):
        """
        Load the model parameters from EMA model.
        Useful for inference or fine-tuning from the EMA model.
        """
        ...
    


