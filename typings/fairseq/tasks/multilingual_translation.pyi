"""
This type stub file was generated by pyright.
"""

from . import LegacyFairseqTask, register_task

logger = ...
@register_task("multilingual_translation")
class MultilingualTranslationTask(LegacyFairseqTask):
    """A task for training multiple translation models simultaneously.

    We iterate round-robin over batches from multiple language pairs, ordered
    according to the `--lang-pairs` argument.

    The training loop is roughly:

        for i in range(len(epoch)):
            for lang_pair in args.lang_pairs:
                batch = next_batch_for_lang_pair(lang_pair)
                loss = criterion(model_for_lang_pair(lang_pair), batch)
                loss.backward()
            optimizer.step()

    In practice, `next_batch_for_lang_pair` is abstracted in a FairseqDataset
    (e.g., `RoundRobinZipDatasets`) and `model_for_lang_pair` is a model that
    implements the `FairseqMultiModel` interface.

    During inference it is required to specify a single `--source-lang` and
    `--target-lang`, which indicates the inference langauge direction.
    `--lang-pairs`, `--encoder-langtok`, `--decoder-langtok` have to be set to
    the same value as training.
    """
    @staticmethod
    def add_args(parser): # -> None:
        """Add task-specific arguments to the parser."""
        ...
    
    def __init__(self, args, dicts, training) -> None:
        ...
    
    @classmethod
    def setup_task(cls, args, **kwargs): # -> Self:
        ...
    
    @classmethod
    def update_args(cls, args): # -> None:
        ...
    
    @classmethod
    def prepare(cls, args, **kargs): # -> tuple[OrderedDict[Any, Any], bool]:
        ...
    
    def get_encoder_langtok(self, src_lang, tgt_lang): # -> int:
        ...
    
    def get_decoder_langtok(self, tgt_lang): # -> int:
        ...
    
    def alter_dataset_langtok(self, lang_pair_dataset, src_eos=..., src_lang=..., tgt_eos=..., tgt_lang=...): # -> TransformEosLangPairDataset:
        ...
    
    def load_dataset(self, split, epoch=..., **kwargs): # -> None:
        """Load a dataset split."""
        ...
    
    def build_dataset_for_inference(self, src_tokens, src_lengths, constraints=...): # -> RoundRobinZipDatasets:
        ...
    
    def build_model(self, args, from_checkpoint=...): # -> FairseqMultiModel:
        ...
    
    def train_step(self, sample, model, criterion, optimizer, update_num, ignore_grad=...): # -> tuple[float | Any, float | Any, defaultdict[Any, float]]:
        ...
    
    def valid_step(self, sample, model, criterion): # -> tuple[float | Any, float | Any, defaultdict[Any, float]]:
        ...
    
    def inference_step(self, generator, models, sample, prefix_tokens=..., constraints=...):
        ...
    
    def reduce_metrics(self, logging_outputs, criterion): # -> None:
        ...
    
    @property
    def source_dictionary(self):
        ...
    
    @property
    def target_dictionary(self):
        ...
    
    def max_positions(self): # -> dict[str, tuple[Any, Any]] | OrderedDict[Any, tuple[Any, Any]]:
        """Return the max sentence length allowed by the task."""
        ...
    


