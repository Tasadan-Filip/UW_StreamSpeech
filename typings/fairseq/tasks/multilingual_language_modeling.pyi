"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from typing import Optional
from fairseq.data.indexed_dataset import get_available_dataset_impl
from fairseq.dataclass import ChoiceEnum, FairseqDataclass
from fairseq.tasks import LegacyFairseqTask, register_task

SAMPLE_BREAK_MODE_CHOICES = ...
SHORTEN_METHOD_CHOICES = ...
logger = ...
def lang_token(lang): # -> str:
    ...

@dataclass
class MultilingualLanguageModelingConfig(FairseqDataclass):
    data: Optional[str] = ...
    sample_break_mode: SAMPLE_BREAK_MODE_CHOICES = ...
    tokens_per_sample: int = ...
    output_dictionary_size: int = ...
    self_target: bool = ...
    future_target: bool = ...
    past_target: bool = ...
    add_bos_token: bool = ...
    max_source_positions: Optional[int] = ...
    max_target_positions: Optional[int] = ...
    pad_to_fixed_length: Optional[bool] = ...
    pad_to_fixed_bsz: Optional[bool] = ...
    multilang_sampling_alpha: Optional[float] = ...
    shorten_method: SHORTEN_METHOD_CHOICES = ...
    shorten_data_split_list: str = ...
    langs: str = ...
    baseline_model_langs: str = ...
    baseline_model: str = ...
    lang_to_offline_shard_ratio: str = ...
    seed: int = ...
    dataset_impl: Optional[ChoiceEnum(get_available_dataset_impl())] = ...
    data_buffer_size: int = ...
    tpu: bool = ...
    batch_size: Optional[int] = ...
    batch_size_valid: Optional[int] = ...
    train_subset: str = ...
    valid_subset: str = ...


@register_task("multilingual_language_modeling", dataclass=MultilingualLanguageModelingConfig)
class MultilingualLanguageModelingTask(LegacyFairseqTask):
    """
    Train a language model.

    Args:
        dictionary (~fairseq.data.Dictionary): the dictionary for the input of
            the language model
        output_dictionary (~fairseq.data.Dictionary): the dictionary for the
            output of the language model. In most cases it will be the same as
            *dictionary*, but could possibly be a more limited version of the
            dictionary (if ``--output-dictionary-size`` is used).
        targets (List[str]): list of the target types that the language model
            should predict.  Can be one of "self", "future", and "past".
            Defaults to "future".

    .. note::

        The language modeling task is compatible with :mod:`fairseq-train`,
        :mod:`fairseq-generate`, :mod:`fairseq-interactive` and
        :mod:`fairseq-eval-lm`.

    The language modeling task provides the following additional command-line
    arguments:

    .. argparse::
        :ref: fairseq.tasks.language_modeling_parser
        :prog:
    """
    def __init__(self, args, dictionary, output_dictionary=..., targets=...) -> None:
        ...
    
    @classmethod
    def setup_dictionary(cls, args, **kwargs): # -> tuple[Dictionary | None, TruncatedDictionary | Dictionary | None]:
        ...
    
    @classmethod
    def setup_task(cls, args, **kwargs): # -> Self:
        """Setup the task (e.g., load dictionaries).

        Args:
            args (argparse.Namespace): parsed command-line arguments
        """
        ...
    
    def build_model(self, args, from_checkpoint=...):
        ...
    
    def load_dataset(self, split: str, epoch=..., combine=..., **kwargs): # -> None:
        """Load a given dataset split.

        Args:
            split (str): name of the split (e.g., train, valid, test)
        """
        ...
    
    def build_dataset_for_inference(self, src_tokens, src_lengths, language=..., **kwargs): # -> NestedDictionaryDataset:
        """
        Generate batches for inference. We prepend an eos token to src_tokens
        (or bos if `--add-bos-token` is set) and we append a <pad> to target.
        This is convenient both for generation with a prefix and LM scoring.
        """
        ...
    
    @torch.no_grad()
    def inference_step(self, generator, models, sample, language=..., prefix_tokens=..., constraints=...):
        ...
    
    def eval_lm_dataloader(self, dataset, max_tokens: Optional[int] = ..., batch_size: Optional[int] = ..., max_positions: Optional[int] = ..., num_shards: int = ..., shard_id: int = ..., num_workers: int = ..., data_buffer_size: int = ..., context_window: int = ...): # -> EpochBatchIterator:
        ...
    
    @property
    def source_dictionary(self): # -> Any:
        """Return the :class:`~fairseq.data.Dictionary` for the language
        model."""
        ...
    
    @property
    def target_dictionary(self): # -> Any:
        """Return the :class:`~fairseq.data.Dictionary` for the language
        model."""
        ...
    


