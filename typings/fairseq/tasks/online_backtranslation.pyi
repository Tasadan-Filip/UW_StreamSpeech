"""
This type stub file was generated by pyright.
"""

import contextlib
import torch
import torch.nn as nn
import fairseq
from typing import Sequence, Tuple
from fairseq.data import FairseqDataset
from fairseq.tasks import register_task
from fairseq.tasks.translation import TranslationTask

logger = ...
class PiecewiseLinearFn:
    """Piecewise linear function. Can be configured with a string."""
    def __init__(self, pieces: Sequence[Tuple[int, float]]) -> None:
        ...
    
    def __call__(self, x: int) -> float:
        ...
    
    @staticmethod
    def from_string(configuration: str) -> PiecewiseLinearFn:
        """
        Parse the configuration of lambda coefficient (for scheduling).
        x = "3"                  # lambda will be a constant equal to x
        x = "0:1,1000:0"         # lambda will start from 1 and linearly decrease
                                 # to 0 during the first 1000 iterations
        x = "0:0,1000:0,2000:1"  # lambda will be equal to 0 for the first 1000
                                 # iterations, then will linearly increase to 1 until iteration 2000
        """
        ...
    
    @staticmethod
    def one() -> PiecewiseLinearFn:
        ...
    


@register_task("online_backtranslation")
class OnlineBackTranslationTask(TranslationTask):
    @staticmethod
    def add_args(parser): # -> None:
        """Add task-specific arguments to the parser."""
        ...
    
    def __init__(self, args, common_dict, mono_langs, valid_lang_pairs) -> None:
        ...
    
    @classmethod
    def setup_task(cls, args, **kwargs): # -> Self:
        """Setup the task (e.g., load dictionaries).

        Args:
            args (argparse.Namespace): parsed command-line arguments
        """
        ...
    
    def load_dataset(self, split, epoch=..., combine=..., **kwargs) -> FairseqDataset:
        """Load a given dataset split.

        Args:
            split (str): name of the split (e.g., train, valid, test)
        """
        ...
    
    def load_train_dataset(self, data_path: str) -> FairseqDataset:
        """The training dataset is made of backtranslation dataset and denoising dataset."""
        ...
    
    def load_bt_dataset(self, data_path: str, lang: str) -> FairseqDataset:
        """The BT dataset is generated with (tgt, tgt) pairs.
        The actual translation to a (generated_src, tgt) pair
        is done on the fly during training.
        """
        ...
    
    def load_denoise_dataset(self, data_path: str, lang: str) -> FairseqDataset:
        """Classic denoising dataset"""
        ...
    
    def load_translation_dataset(self, split: str, data_path: str, combine: bool = ...): # -> LanguagePairDataset:
        ...
    
    def build_dataset_for_inference(self, src_tokens, src_lengths, constraints=...):
        ...
    
    def build_model(self, args, from_checkpoint=...):
        ...
    
    def max_positions(self): # -> tuple[Any, Any]:
        """Return the max sentence length allowed by the task."""
        ...
    
    @property
    def dictionary(self): # -> Any:
        """Return the source :class:`~fairseq.data.Dictionary`."""
        ...
    
    def display_samples_once_in_a_while(self, smp, mono_lang, other_lang): # -> None:
        ...
    
    def backtranslate_sample(self, smp, orig_lang, other_lang) -> None:
        """
        * WARNING: smp is modified in place.
        * At the start of this function, `smp` has the same input and target:
          |--------------------------------------------------------|
          | smp['net_input']['src_tokens'] |  smp['target']        |
          | (from data) __en__ hello world |  __en__ hello world   |
          |--------------------------------------------------------|

        * We call generator.generate(smp, bos_token = token("ro")),
        and copy the result as input
        * At the end, `smp` has the translation to other language.
          |--------------------------------------------------------|
          | smp['net_input']['src_tokens'] |  smp['target']        |
          | (generated) __ro__ salut lume  |  __en__ hello world   |
          |--------------------------------------------------------|

        """
        ...
    
    def generate(self, smp, model):
        ...
    
    def get_other_lang(self, lang):
        ...
    
    def train_step(self, sample, model, criterion, optimizer, update_num, ignore_grad=...): # -> tuple[float | Any, float | Any, defaultdict[str, float]]:
        ...
    
    def get_bos_token_from_sample(self, sample): # -> int:
        ...
    
    def reduce_metrics(self, logging_outputs, criterion): # -> None:
        ...
    


@torch.no_grad()
def extend_embedding(emb: nn.Module, new_vocab_size: int, copy_from_token_id: int) -> None:
    ...

def add_secial_tokens_to_dict_and_model(dictionary: fairseq.data.Dictionary, model: nn.Module, mono_langs: Sequence[str]) -> None:
    ...

@contextlib.contextmanager
def assert_weights_have_changed(model: nn.Module): # -> Generator[Module, Any, None]:
    ...

