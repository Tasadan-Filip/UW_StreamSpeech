"""
This type stub file was generated by pyright.
"""

from fairseq.modules import TransformerDecoderLayer, TransformerEncoderLayer

has_megatron_submodule = ...
class ModelParallelTransformerEncoderLayer(TransformerEncoderLayer):
    """Encoder layer block over multiple gpus.

    See "Megatron-LM: https://arxiv.org/pdf/1909.08053.pdf" for more details.
    """
    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):
        ...
    
    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):
        ...
    
    def build_self_attention(self, embed_dim, args, **unused_kwargs): # -> ModelParallelMultiheadAttention:
        ...
    


class ModelParallelTransformerDecoderLayer(TransformerDecoderLayer):
    """Decoder layer block.

    See "Megatron-LM: https://arxiv.org/pdf/1909.08053.pdf" for more details.
    """
    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):
        ...
    
    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):
        ...
    
    def build_self_attention(self, embed_dim, args, **unused_kwargs): # -> ModelParallelMultiheadAttention:
        ...
    
    def build_encoder_attention(self, embed_dim, args, **unused_kwargs): # -> ModelParallelMultiheadAttention:
        ...
    


