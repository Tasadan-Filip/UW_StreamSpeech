"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from typing import Dict, List, Optional
from torch import Tensor

class SequenceGenerator(nn.Module):
    def __init__(self, models, tgt_dict, beam_size=..., max_len_a=..., max_len_b=..., max_len=..., min_len=..., normalize_scores=..., len_penalty=..., unk_penalty=..., temperature=..., match_source_len=..., no_repeat_ngram_size=..., search_strategy=..., eos=..., symbols_to_strip_from_output=..., lm_model=..., lm_weight=..., tokens_to_suppress=...) -> None:
        """Generates translations of a given source sentence.

        Args:
            models (List[~fairseq.models.FairseqModel]): ensemble of models,
                currently support fairseq.models.TransformerModel for scripting
            beam_size (int, optional): beam width (default: 1)
            max_len_a/b (int, optional): generate sequences of maximum length
                ax + b, where x is the source length
            max_len (int, optional): the maximum length of the generated output
                (not including end-of-sentence)
            min_len (int, optional): the minimum length of the generated output
                (not including end-of-sentence)
            normalize_scores (bool, optional): normalize scores by the length
                of the output (default: True)
            len_penalty (float, optional): length penalty, where <1.0 favors
                shorter, >1.0 favors longer sentences (default: 1.0)
            unk_penalty (float, optional): unknown word penalty, where <0
                produces more unks, >0 produces fewer (default: 0.0)
            temperature (float, optional): temperature, where values
                >1.0 produce more uniform samples and values <1.0 produce
                sharper samples (default: 1.0)
            match_source_len (bool, optional): outputs should match the source
                length (default: False)
        """
        ...
    
    def cuda(self): # -> Self:
        ...
    
    @torch.no_grad()
    def forward(self, sample: Dict[str, Dict[str, Tensor]], prefix_tokens: Optional[Tensor] = ..., bos_token: Optional[int] = ...):
        """Generate a batch of translations.

        Args:
            sample (dict): batch
            prefix_tokens (torch.LongTensor, optional): force decoder to begin
                with these tokens
            bos_token (int, optional): beginning of sentence token
                (default: self.eos)
        """
        ...
    
    def generate_batched_itr(self, data_itr, beam_size=..., cuda=..., timer=...): # -> Generator[tuple[Any, Any, Any | None, Any], Any, None]:
        """Iterate over a batched dataset and yield individual translations.
        Args:
            cuda (bool, optional): use GPU for generation
            timer (StopwatchMeter, optional): time generations
        """
        ...
    
    @torch.no_grad()
    def generate(self, models, sample: Dict[str, Dict[str, Tensor]], **kwargs) -> List[List[Dict[str, Tensor]]]:
        """Generate translations. Match the api of other fairseq generators.

        Args:
            models (List[~fairseq.models.FairseqModel]): ensemble of models
            sample (dict): batch
            prefix_tokens (torch.LongTensor, optional): force decoder to begin
                with these tokens
            constraints (torch.LongTensor, optional): force decoder to include
                the list of constraints
            bos_token (int, optional): beginning of sentence token
                (default: self.eos)
        """
        ...
    
    def replicate_first_beam(self, tensor, mask, beam_size: int):
        ...
    
    def finalize_hypos(self, step: int, bbsz_idx, eos_scores, tokens, scores, finalized: List[List[Dict[str, Tensor]]], finished: List[bool], beam_size: int, attn: Optional[Tensor], src_lengths, max_len: int): # -> list[int]:
        """Finalize hypothesis, store finalized information in `finalized`, and change `finished` accordingly.
        A sentence is finalized when {beam_size} finished items have been collected for it.

        Returns number of sentences (not beam items) being finalized.
        These will be removed from the batch and not processed further.
        Args:
            bbsz_idx (Tensor):
        """
        ...
    
    def is_finished(self, step: int, unfin_idx: int, max_len: int, finalized_sent_len: int, beam_size: int): # -> bool:
        """
        Check whether decoding for a sentence is finished, which
        occurs when the list of finalized sentences has reached the
        beam size, or when we reach the maximum length.
        """
        ...
    


class EnsembleModel(nn.Module):
    """A wrapper around an ensemble of models."""
    def __init__(self, models) -> None:
        ...
    
    def forward(self): # -> None:
        ...
    
    def has_encoder(self): # -> bool:
        ...
    
    def has_incremental_states(self): # -> bool:
        ...
    
    def max_decoder_positions(self): # -> Any:
        ...
    
    def set_decoder_beam_size(self, beam_size): # -> None:
        """Set beam size for efficient beamable enc-dec attention."""
        ...
    
    @torch.jit.export
    def forward_encoder(self, net_input: Dict[str, Tensor]): # -> list[Any] | None:
        ...
    
    @torch.jit.export
    def forward_decoder(self, tokens, encoder_outs: List[Dict[str, List[Tensor]]], incremental_states: List[Dict[str, Dict[str, Optional[Tensor]]]], temperature: float = ...): # -> tuple[Any, Tensor | Any | None] | tuple[Tensor, Tensor | Any | None]:
        ...
    
    @torch.jit.export
    def reorder_encoder_out(self, encoder_outs: Optional[List[Dict[str, List[Tensor]]]], new_order): # -> list[Dict[str, List[Tensor]]]:
        """
        Reorder encoder output according to *new_order*.

        Args:
            encoder_out: output from the ``forward()`` method
            new_order (LongTensor): desired order

        Returns:
            *encoder_out* rearranged according to *new_order*
        """
        ...
    
    @torch.jit.export
    def reorder_incremental_state(self, incremental_states: List[Dict[str, Dict[str, Optional[Tensor]]]], new_order): # -> None:
        ...
    


class SequenceGeneratorWithAlignment(SequenceGenerator):
    def __init__(self, models, tgt_dict, left_pad_target=..., print_alignment=..., **kwargs) -> None:
        """Generates translations of a given source sentence.

        Produces alignments following "Jointly Learning to Align and
        Translate with Transformer Models" (Garg et al., EMNLP 2019).

        Args:
            left_pad_target (bool, optional): Whether or not the
                hypothesis should be left padded or not when they are
                teacher forced for generating alignments.
        """
        ...
    
    @torch.no_grad()
    def generate(self, models, sample, **kwargs):
        ...
    


class EnsembleModelWithAlignment(EnsembleModel):
    """A wrapper around an ensemble of models."""
    def __init__(self, models) -> None:
        ...
    
    def forward_align(self, src_tokens, src_lengths, prev_output_tokens): # -> Any | None:
        ...
    


