"""
This type stub file was generated by pyright.
"""

from typing import Any, Dict, Optional, Union
from fairseq.dataclass.configs import CheckpointConfig
from fairseq.models import FairseqDecoder, FairseqEncoder
from omegaconf import DictConfig

logger = ...
def save_checkpoint(cfg: CheckpointConfig, trainer, epoch_itr, val_loss): # -> None:
    ...

def load_checkpoint(cfg: CheckpointConfig, trainer, **passthrough_args): # -> tuple[Any, Any]:
    """
    Load a checkpoint and restore the training iterator.

    *passthrough_args* will be passed through to
    ``trainer.get_train_iterator``.
    """
    ...

def load_checkpoint_to_cpu(path, arg_overrides=..., load_on_all_ranks=...):
    """Loads a checkpoint to CPU (with upgrading for backward compatibility).

    If doing single-GPU training or if the checkpoint is only being loaded by at
    most one process on each node (current default behavior is for only rank 0
    to read the checkpoint from disk), load_on_all_ranks should be False to
    avoid errors from torch.distributed not having been initialized or
    torch.distributed.barrier() hanging.

    If all processes on each node may be loading the checkpoint
    simultaneously, load_on_all_ranks should be set to True to avoid I/O
    conflicts.

    There's currently no support for > 1 but < all processes loading the
    checkpoint on each node.
    """
    ...

def load_model_ensemble(filenames, arg_overrides: Optional[Dict[str, Any]] = ..., task=..., strict=..., suffix=..., num_shards=..., state=...): # -> tuple[list[Any], DictConfig | Any | None]:
    """Loads an ensemble of models.

    Args:
        filenames (List[str]): checkpoint files to load
        arg_overrides (Dict[str,Any], optional): override model args that
            were used during model training
        task (fairseq.tasks.FairseqTask, optional): task to use for loading
    """
    ...

def get_maybe_sharded_checkpoint_filename(filename: str, suffix: str, shard_idx: int, num_shards: int) -> str:
    ...

def load_model_ensemble_and_task(filenames, arg_overrides: Optional[Dict[str, Any]] = ..., task=..., strict=..., suffix=..., num_shards=..., state=...): # -> tuple[list[Any], DictConfig | Any | None, Any | None]:
    ...

def load_model_ensemble_and_task_from_hf_hub(model_id, cache_dir: Optional[str] = ..., arg_overrides: Optional[Dict[str, Any]] = ..., **kwargs: Any): # -> tuple[list[Any], DictConfig | Any | None, Any | None]:
    ...

def checkpoint_paths(path, pattern=..., keep_match=...): # -> list[tuple[Any, Any]] | list[Any]:
    """Retrieves all checkpoints found in `path` directory.

    Checkpoints are identified by matching filename to the specified pattern. If
    the pattern contains groups, the result will be sorted by the first group in
    descending order.
    """
    ...

def torch_persistent_save(obj, filename, async_write: bool = ...): # -> None:
    ...

def prune_state_dict(state_dict, model_cfg: Optional[DictConfig]): # -> dict[Any, Any]:
    """Prune the given state_dict if desired for LayerDrop
    (https://arxiv.org/abs/1909.11556).

    Training with LayerDrop allows models to be robust to pruning at inference
    time. This function prunes state_dict to allow smaller models to be loaded
    from a larger model and re-maps the existing state_dict for this to occur.

    It's called by functions that load models from checkpoints and does not
    need to be called directly.
    """
    ...

def load_pretrained_component_from_model(component: Union[FairseqEncoder, FairseqDecoder], checkpoint: str, strict: bool = ...): # -> FairseqEncoder | FairseqDecoder:
    """
    Load a pretrained FairseqEncoder or FairseqDecoder from checkpoint into the
    provided `component` object. If state_dict fails to load, there may be a
    mismatch in the architecture of the corresponding `component` found in the
    `checkpoint` file.
    """
    ...

def verify_checkpoint_directory(save_dir: str) -> None:
    ...

def save_ema_as_checkpoint(src_path, dst_path): # -> None:
    ...

def load_ema_from_checkpoint(fpath): # -> Any:
    """Loads exponential moving averaged (EMA) checkpoint from input and
    returns a model with ema weights.

    Args:
      fpath: A string path of checkpoint to load from.

    Returns:
      A dict of string keys mapping to various values. The 'model' key
      from the returned dict should correspond to an OrderedDict mapping
      string parameter names to torch Tensors.
    """
    ...

