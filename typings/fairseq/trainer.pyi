"""
This type stub file was generated by pyright.
"""

from fairseq.dataclass.configs import FairseqConfig
from fairseq.logging import metrics

"""
Train a network across multiple GPUs.
"""
logger = ...
class Trainer:
    """Main class for data parallel training.

    This class supports synchronous distributed data parallel training,
    where multiple workers each have a full model replica and gradients
    are accumulated across workers before each update. We use
    :class:`~torch.nn.parallel.DistributedDataParallel` to handle
    communication of the gradients across workers.
    """
    def __init__(self, cfg: FairseqConfig, task, model, criterion, quantizer=...) -> None:
        ...
    
    def reinitialize(self): # -> None:
        """Reinitialize the Trainer, typically after model params change."""
        ...
    
    @property
    def data_parallel_world_size(self): # -> int:
        ...
    
    @property
    def data_parallel_process_group(self): # -> tuple[Literal['tpu'], List[List[int]]] | object | ProcessGroupMPI | ProcessGroupGloo | ProcessGroupNCCL | ProcessGroupUCC | ProcessGroup | Any | None:
        ...
    
    @property
    def data_parallel_rank(self): # -> int:
        ...
    
    @property
    def is_data_parallel_master(self): # -> bool:
        ...
    
    @property
    def use_distributed_wrapper(self) -> bool:
        ...
    
    @property
    def should_save_checkpoint_on_current_rank(self) -> bool:
        """Indicates whether to save checkpoints on the current DDP rank."""
        ...
    
    @property
    def always_call_state_dict_during_save_checkpoint(self) -> bool:
        ...
    
    @property
    def checkpoint_suffix(self) -> str:
        """Suffix to add to the checkpoint file name."""
        ...
    
    @property
    def criterion(self): # -> DistributedTimeoutWrapper | ModuleProxyWrapper | Module:
        ...
    
    @property
    def model(self): # -> DistributedTimeoutWrapper | ModuleProxyWrapper | Module:
        ...
    
    @property
    def ema(self): # -> EMA | None:
        ...
    
    @property
    def optimizer(self): # -> MemoryEfficientFP16Optimizer | AMPOptimizer | FP16Optimizer | Any | FairseqBMUF | None:
        ...
    
    @property
    def lr_scheduler(self): # -> Any | None:
        ...
    
    @property
    def is_fsdp(self):
        ...
    
    def consolidate_optimizer(self): # -> None:
        """For OSS, we need to consolidate the state dict."""
        ...
    
    def state_dict(self): # -> dict[str, Dict[str, Any] | List[Any] | str | FairseqConfig | Any | list[Any] | dict[str, OrderedDict[Any, Any] | float | Any] | None]:
        ...
    
    def save_checkpoint(self, filename, extra_state): # -> None:
        """Save all training state in a checkpoint file."""
        ...
    
    def load_checkpoint(self, filename, reset_optimizer=..., reset_lr_scheduler=..., optimizer_overrides=..., reset_meters=...):
        """
        Load all training state from a checkpoint file.
        rank = 0 will load the checkpoint, and then broadcast it to all
        other ranks.
        """
        ...
    
    def get_train_iterator(self, epoch, combine=..., load_dataset=..., data_selector=..., shard_batch_itr=..., disable_iterator_cache=...):
        """Return an EpochBatchIterator over the training set for a given epoch."""
        ...
    
    def get_valid_iterator(self, subset, disable_iterator_cache=...):
        """Return an EpochBatchIterator over given validation subset for a given epoch."""
        ...
    
    def begin_epoch(self, epoch): # -> None:
        """Called at the beginning of each epoch."""
        ...
    
    def begin_valid_epoch(self, epoch): # -> None:
        """Called at the beginning of each validation epoch."""
        ...
    
    def reset_dummy_batch(self, batch): # -> None:
        ...
    
    @metrics.aggregate("train")
    def train_step(self, samples, raise_oom=...):
        """Do forward, backward and parameter update."""
        ...
    
    @metrics.aggregate("valid")
    def valid_step(self, sample, raise_oom=...): # -> Dict[str, float]:
        """Do forward pass in evaluation mode."""
        ...
    
    def zero_grad(self): # -> None:
        ...
    
    def lr_step_begin_epoch(self, epoch): # -> Any:
        """Adjust the learning rate at the beginning of the epoch."""
        ...
    
    def lr_step(self, epoch, val_loss=...): # -> Any:
        """Adjust the learning rate at the end of the epoch."""
        ...
    
    def lr_step_update(self): # -> Any:
        """Update the learning rate after each update."""
        ...
    
    def get_lr(self): # -> Any:
        """Get the current learning rate."""
        ...
    
    def get_model(self):
        """Get the (non-wrapped) model instance."""
        ...
    
    def get_criterion(self):
        """Get the (non-wrapped) criterion instance."""
        ...
    
    def get_meter(self, name): # -> AverageMeter | Meter | None:
        """[deprecated] Get a specific meter by name."""
        ...
    
    def get_num_updates(self): # -> int:
        """Get the number of parameters updates."""
        ...
    
    def set_num_updates(self, num_updates): # -> None:
        """Set the number of parameters updates."""
        ...
    
    def clip_grad_norm(self, clip_norm): # -> Any:
        ...
    
    def cumulative_training_time(self): # -> float | Any:
        ...
    


