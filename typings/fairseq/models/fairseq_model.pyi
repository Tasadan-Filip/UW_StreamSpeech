"""
This type stub file was generated by pyright.
"""

import torch.nn as nn
from argparse import Namespace
from typing import Dict, List, Optional, Tuple
from fairseq.data import Dictionary
from omegaconf import DictConfig
from torch import Tensor

"""
Base classes for various fairseq models.
"""
logger = ...
def check_type(module, expected_type): # -> None:
    ...

class BaseFairseqModel(nn.Module):
    """Base class for fairseq models."""
    def __init__(self) -> None:
        ...
    
    @classmethod
    def add_args(cls, parser): # -> None:
        """Add model-specific arguments to the parser."""
        ...
    
    @classmethod
    def build_model(cls, args, task):
        """Build a new model instance."""
        ...
    
    def get_targets(self, sample, net_output):
        """Get targets from either the sample or the net's output."""
        ...
    
    def get_normalized_probs(self, net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]], log_probs: bool, sample: Optional[Dict[str, Tensor]] = ...): # -> Any | Tensor:
        """Get normalized probabilities (or log probs) from a net's output."""
        ...
    
    def get_normalized_probs_scriptable(self, net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]], log_probs: bool, sample: Optional[Dict[str, Tensor]] = ...): # -> Any | Tensor:
        """Scriptable helper function for get_normalized_probs in ~BaseFairseqModel"""
        ...
    
    def extract_features(self, *args, **kwargs): # -> Any:
        """Similar to *forward* but only return features."""
        ...
    
    def max_positions(self): # -> None:
        """Maximum length supported by the model."""
        ...
    
    def load_state_dict(self, state_dict, strict=..., model_cfg: Optional[DictConfig] = ..., args: Optional[Namespace] = ...): # -> _IncompatibleKeys:
        """Copies parameters and buffers from *state_dict* into this module and
        its descendants.

        Overrides the method in :class:`nn.Module`. Compared with that method
        this additionally "upgrades" *state_dicts* from old checkpoints.
        """
        ...
    
    def upgrade_state_dict(self, state_dict): # -> None:
        """Upgrade old state dicts to work with newer code."""
        ...
    
    def upgrade_state_dict_named(self, state_dict, name): # -> None:
        """Upgrade old state dicts to work with newer code.

        Args:
            state_dict (dict): state dictionary to upgrade, in place
            name (str): the state dict key corresponding to the current module
        """
        ...
    
    def set_num_updates(self, num_updates): # -> None:
        """State from trainer to pass along to model at every update."""
        ...
    
    def prepare_for_inference_(self, cfg: DictConfig): # -> None:
        """Prepare model for inference."""
        ...
    
    def make_generation_fast_(self, **kwargs): # -> None:
        """
        Legacy entry point to optimize model for faster generation.
        Prefer prepare_for_inference_.
        """
        ...
    
    def prepare_for_onnx_export_(self, **kwargs): # -> None:
        """Make model exportable via ONNX trace."""
        ...
    
    @classmethod
    def from_pretrained(cls, model_name_or_path, checkpoint_file=..., data_name_or_path=..., **kwargs): # -> GeneratorHubInterface:
        """
        Load a :class:`~fairseq.models.FairseqModel` from a pre-trained model
        file. Downloads and caches the pre-trained model file if needed.

        The base implementation returns a
        :class:`~fairseq.hub_utils.GeneratorHubInterface`, which can be used to
        generate translations or sample from language models. The underlying
        :class:`~fairseq.models.FairseqModel` can be accessed via the
        *generator.models* attribute.

        Other models may override this to implement custom hub interfaces.

        Args:
            model_name_or_path (str): either the name of a pre-trained model to
                load or a path/URL to a pre-trained model state dict
            checkpoint_file (str, optional): colon-separated list of checkpoint
                files in the model archive to ensemble (default: 'model.pt')
            data_name_or_path (str, optional): point args.data to the archive
                at the given path/URL. Can start with '.' or './' to reuse the
                model archive path.
        """
        ...
    
    @classmethod
    def hub_models(cls): # -> dict[Any, Any]:
        ...
    


class FairseqEncoderDecoderModel(BaseFairseqModel):
    """Base class for encoder-decoder models.

    Args:
        encoder (FairseqEncoder): the encoder
        decoder (FairseqDecoder): the decoder
    """
    def __init__(self, encoder, decoder) -> None:
        ...
    
    def forward(self, src_tokens, src_lengths, prev_output_tokens, **kwargs):
        """
        Run the forward pass for an encoder-decoder model.

        First feed a batch of source tokens through the encoder. Then, feed the
        encoder output and previous decoder outputs (i.e., teacher forcing) to
        the decoder to produce the next outputs::

            encoder_out = self.encoder(src_tokens, src_lengths)
            return self.decoder(prev_output_tokens, encoder_out)

        Args:
            src_tokens (LongTensor): tokens in the source language of shape
                `(batch, src_len)`
            src_lengths (LongTensor): source sentence lengths of shape `(batch)`
            prev_output_tokens (LongTensor): previous decoder outputs of shape
                `(batch, tgt_len)`, for teacher forcing

        Returns:
            tuple:
                - the decoder's output of shape `(batch, tgt_len, vocab)`
                - a dictionary with any model-specific outputs
        """
        ...
    
    def forward_decoder(self, prev_output_tokens, **kwargs):
        ...
    
    def extract_features(self, src_tokens, src_lengths, prev_output_tokens, **kwargs):
        """
        Similar to *forward* but only return features.

        Returns:
            tuple:
                - the decoder's features of shape `(batch, tgt_len, embed_dim)`
                - a dictionary with any model-specific outputs
        """
        ...
    
    def output_layer(self, features, **kwargs):
        """Project features to the default output size (typically vocabulary size)."""
        ...
    
    def max_positions(self): # -> tuple[Any, Any]:
        """Maximum length supported by the model."""
        ...
    
    def max_decoder_positions(self):
        """Maximum length supported by the decoder."""
        ...
    


class FairseqModel(FairseqEncoderDecoderModel):
    def __init__(self, *args, **kwargs) -> None:
        ...
    


class FairseqMultiModel(BaseFairseqModel):
    """Base class for combining multiple encoder-decoder models."""
    def __init__(self, encoders, decoders) -> None:
        ...
    
    @staticmethod
    def build_shared_embeddings(dicts: Dict[str, Dictionary], langs: List[str], embed_dim: int, build_embedding: callable, pretrained_embed_path: Optional[str] = ...):
        """
        Helper function to build shared embeddings for a set of languages after
        checking that all dicts corresponding to those languages are equivalent.

        Args:
            dicts: Dict of lang_id to its corresponding Dictionary
            langs: languages that we want to share embeddings for
            embed_dim: embedding dimension
            build_embedding: callable function to actually build the embedding
            pretrained_embed_path: Optional path to load pretrained embeddings
        """
        ...
    
    def forward(self, src_tokens, src_lengths, prev_output_tokens, **kwargs):
        ...
    
    def max_positions(self): # -> dict[Any, tuple[Any, Any]]:
        """Maximum length supported by the model."""
        ...
    
    def max_decoder_positions(self): # -> Any:
        """Maximum length supported by the decoder."""
        ...
    
    @property
    def encoder(self): # -> Tensor | Module:
        ...
    
    @property
    def decoder(self): # -> Tensor | Module:
        ...
    
    def forward_decoder(self, prev_output_tokens, **kwargs): # -> Any:
        ...
    
    def load_state_dict(self, state_dict, strict=..., model_cfg=..., args: Optional[Namespace] = ...): # -> _IncompatibleKeys:
        """Copies parameters and buffers from *state_dict* into this module and
        its descendants.

        Overrides the method in :class:`nn.Module`. Compared with that method
        this additionally "upgrades" *state_dicts* from old checkpoints.
        """
        ...
    


class FairseqLanguageModel(BaseFairseqModel):
    """Base class for decoder-only models.

    Args:
        decoder (FairseqDecoder): the decoder
    """
    def __init__(self, decoder) -> None:
        ...
    
    def forward(self, src_tokens, **kwargs):
        """
        Run the forward pass for a decoder-only model.

        Feeds a batch of tokens through the decoder to predict the next tokens.

        Args:
            src_tokens (LongTensor): tokens on which to condition the decoder,
                of shape `(batch, tgt_len)`
            src_lengths (LongTensor): source sentence lengths of shape `(batch)`

        Returns:
            tuple:
                - the decoder's output of shape `(batch, seq_len, vocab)`
                - a dictionary with any model-specific outputs
        """
        ...
    
    def forward_decoder(self, prev_output_tokens, **kwargs):
        ...
    
    def extract_features(self, src_tokens, **kwargs):
        """
        Similar to *forward* but only return features.

        Returns:
            tuple:
                - the decoder's features of shape `(batch, seq_len, embed_dim)`
                - a dictionary with any model-specific outputs
        """
        ...
    
    def output_layer(self, features, **kwargs):
        """Project features to the default output size (typically vocabulary size)."""
        ...
    
    def max_positions(self):
        """Maximum length supported by the model."""
        ...
    
    def max_decoder_positions(self):
        """Maximum length supported by the decoder."""
        ...
    
    @property
    def supported_targets(self): # -> set[str]:
        ...
    


class FairseqEncoderModel(BaseFairseqModel):
    """Base class for encoder-only models.

    Args:
        encoder (FairseqEncoder): the encoder
    """
    def __init__(self, encoder) -> None:
        ...
    
    def forward(self, src_tokens, src_lengths, **kwargs):
        """
        Run the forward pass for a encoder-only model.

        Feeds a batch of tokens through the encoder to generate features.

        Args:
            src_tokens (LongTensor): input tokens of shape `(batch, src_len)`
            src_lengths (LongTensor): source sentence lengths of shape `(batch)`

        Returns:
            the encoder's output, typically of shape `(batch, src_len, features)`
        """
        ...
    
    def get_normalized_probs(self, net_output, log_probs, sample=...): # -> Tensor:
        """Get normalized probabilities (or log probs) from a net's output."""
        ...
    
    def max_positions(self):
        """Maximum length supported by the model."""
        ...
    


