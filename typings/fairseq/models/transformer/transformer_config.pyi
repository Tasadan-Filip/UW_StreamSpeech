"""
This type stub file was generated by pyright.
"""

from dataclasses import dataclass
from typing import List, Optional
from fairseq import utils
from fairseq.dataclass import ChoiceEnum, FairseqDataclass

DEFAULT_MAX_SOURCE_POSITIONS = ...
DEFAULT_MAX_TARGET_POSITIONS = ...
DEFAULT_MIN_PARAMS_TO_WRAP = ...
_NAME_PARSER = ...
@dataclass
class EncDecBaseConfig(FairseqDataclass):
    embed_path: Optional[str] = ...
    embed_dim: Optional[int] = ...
    ffn_embed_dim: int = ...
    layers: int = ...
    attention_heads: int = ...
    normalize_before: bool = ...
    learned_pos: bool = ...
    layerdrop: float = ...
    layers_to_keep: Optional[List[int]] = ...
    xformers_att_config: Optional[str] = ...


@dataclass
class DecoderConfig(EncDecBaseConfig):
    input_dim: int = ...
    output_dim: int = ...
    def __post_init__(self): # -> None:
        ...
    


@dataclass
class QuantNoiseConfig(FairseqDataclass):
    pq: float = ...
    pq_block_size: int = ...
    scalar: float = ...


@dataclass
class TransformerConfig(FairseqDataclass):
    activation_fn: ChoiceEnum(utils.get_available_activation_fns()) = ...
    dropout: float = ...
    attention_dropout: float = ...
    activation_dropout: float = ...
    adaptive_input: bool = ...
    encoder: EncDecBaseConfig = ...
    max_source_positions: int = ...
    decoder: DecoderConfig = ...
    max_target_positions: int = ...
    share_decoder_input_output_embed: bool = ...
    share_all_embeddings: bool = ...
    merge_src_tgt_embed: bool = ...
    no_token_positional_embeddings: bool = ...
    adaptive_softmax_cutoff: Optional[List[int]] = ...
    adaptive_softmax_dropout: float = ...
    adaptive_softmax_factor: float = ...
    layernorm_embedding: bool = ...
    tie_adaptive_weights: bool = ...
    tie_adaptive_proj: bool = ...
    no_scale_embedding: bool = ...
    checkpoint_activations: bool = ...
    offload_activations: bool = ...
    no_cross_attention: bool = ...
    cross_self_attention: bool = ...
    quant_noise: QuantNoiseConfig = ...
    min_params_to_wrap: int = ...
    char_inputs: bool = ...
    relu_dropout: float = ...
    base_layers: Optional[int] = ...
    base_sublayers: Optional[int] = ...
    base_shuffle: Optional[int] = ...
    export: bool = ...
    no_decoder_final_norm: bool = ...
    def __getattr__(self, name): # -> Any | None:
        ...
    
    def __setattr__(self, name, value): # -> None:
        ...
    
    @classmethod
    def from_namespace(cls, args): # -> Self | None:
        ...
    


