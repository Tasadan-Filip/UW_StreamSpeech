"""
This type stub file was generated by pyright.
"""

import torch
from typing import Dict, List, Optional
from torch import Tensor
from fairseq.models import FairseqEncoder

def module_name_fordropout(module_name: str) -> str:
    ...

class TransformerEncoderBase(FairseqEncoder):
    """
    Transformer encoder consisting of *cfg.encoder.layers* layers. Each layer
    is a :class:`TransformerEncoderLayer`.

    Args:
        args (argparse.Namespace): parsed command-line arguments
        dictionary (~fairseq.data.Dictionary): encoding dictionary
        embed_tokens (torch.nn.Embedding): input embedding
    """
    def __init__(self, cfg, dictionary, embed_tokens, return_fc=...) -> None:
        ...
    
    def build_encoder_layer(self, cfg):
        ...
    
    def forward_embedding(self, src_tokens, token_embedding: Optional[torch.Tensor] = ...): # -> tuple[Any, Any | Tensor]:
        ...
    
    def forward(self, src_tokens, src_lengths: Optional[torch.Tensor] = ..., return_all_hiddens: bool = ..., token_embeddings: Optional[torch.Tensor] = ...): # -> dict[str, list[Any | Tensor | tuple[Any, ...]] | list[Any] | list[Any | Tensor] | list[Any | Tensor | None]]:
        """
        Args:
            src_tokens (LongTensor): tokens in the source language of shape
                `(batch, src_len)`
            src_lengths (torch.LongTensor): lengths of each source sentence of
                shape `(batch)`
            return_all_hiddens (bool, optional): also return all of the
                intermediate hidden states (default: False).
            token_embeddings (torch.Tensor, optional): precomputed embeddings
                default `None` will recompute embeddings

        Returns:
            dict:
                - **encoder_out** (Tensor): the last encoder layer's output of
                  shape `(src_len, batch, embed_dim)`
                - **encoder_padding_mask** (ByteTensor): the positions of
                  padding elements of shape `(batch, src_len)`
                - **encoder_embedding** (Tensor): the (scaled) embedding lookup
                  of shape `(batch, src_len, embed_dim)`
                - **encoder_states** (List[Tensor]): all intermediate
                  hidden states of shape `(src_len, batch, embed_dim)`.
                  Only populated if *return_all_hiddens* is True.
        """
        ...
    
    def forward_scriptable(self, src_tokens, src_lengths: Optional[torch.Tensor] = ..., return_all_hiddens: bool = ..., token_embeddings: Optional[torch.Tensor] = ...): # -> dict[str, list[Any | Tensor | tuple[Any, ...]] | list[Any] | list[Any | Tensor] | list[Any | Tensor | None]]:
        """
        Args:
            src_tokens (LongTensor): tokens in the source language of shape
                `(batch, src_len)`
            src_lengths (torch.LongTensor): lengths of each source sentence of
                shape `(batch)`
            return_all_hiddens (bool, optional): also return all of the
                intermediate hidden states (default: False).
            token_embeddings (torch.Tensor, optional): precomputed embeddings
                default `None` will recompute embeddings

        Returns:
            dict:
                - **encoder_out** (Tensor): the last encoder layer's output of
                  shape `(src_len, batch, embed_dim)`
                - **encoder_padding_mask** (ByteTensor): the positions of
                  padding elements of shape `(batch, src_len)`
                - **encoder_embedding** (Tensor): the (scaled) embedding lookup
                  of shape `(batch, src_len, embed_dim)`
                - **encoder_states** (List[Tensor]): all intermediate
                  hidden states of shape `(src_len, batch, embed_dim)`.
                  Only populated if *return_all_hiddens* is True.
        """
        ...
    
    @torch.jit.export
    def reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order): # -> dict[str, list[Tensor]]:
        """
        Reorder encoder output according to *new_order*.

        Args:
            encoder_out: output from the ``forward()`` method
            new_order (LongTensor): desired order

        Returns:
            *encoder_out* rearranged according to *new_order*
        """
        ...
    
    def max_positions(self): # -> int:
        """Maximum input length supported by the encoder."""
        ...
    
    def upgrade_state_dict_named(self, state_dict, name):
        """Upgrade a (possibly old) state dict for new versions of fairseq."""
        ...
    


class TransformerEncoder(TransformerEncoderBase):
    def __init__(self, args, dictionary, embed_tokens, return_fc=...) -> None:
        ...
    
    def build_encoder_layer(self, args):
        ...
    


