"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from dataclasses import dataclass
from typing import List, Tuple
from fairseq import utils
from fairseq.dataclass import ChoiceEnum, FairseqDataclass
from fairseq.models import BaseFairseqModel, register_model

EXTRACTOR_MODE_CHOICES = ...
MASKING_DISTRIBUTION_CHOICES = ...
LAYER_TYPE_CHOICES = ...
@dataclass
class Wav2Vec2Config(FairseqDataclass):
    extractor_mode: EXTRACTOR_MODE_CHOICES = ...
    encoder_layers: int = ...
    encoder_embed_dim: int = ...
    encoder_ffn_embed_dim: int = ...
    encoder_attention_heads: int = ...
    activation_fn: ChoiceEnum(utils.get_available_activation_fns()) = ...
    layer_type: LAYER_TYPE_CHOICES = ...
    dropout: float = ...
    attention_dropout: float = ...
    activation_dropout: float = ...
    encoder_layerdrop: float = ...
    dropout_input: float = ...
    dropout_features: float = ...
    final_dim: int = ...
    layer_norm_first: bool = ...
    conv_feature_layers: str = ...
    conv_bias: bool = ...
    logit_temp: float = ...
    quantize_targets: bool = ...
    quantize_input: bool = ...
    same_quantizer: bool = ...
    target_glu: bool = ...
    feature_grad_mult: float = ...
    quantizer_depth: int = ...
    quantizer_factor: int = ...
    latent_vars: int = ...
    latent_groups: int = ...
    latent_dim: int = ...
    mask_length: int = ...
    mask_prob: float = ...
    mask_selection: MASKING_DISTRIBUTION_CHOICES = ...
    mask_other: float = ...
    no_mask_overlap: bool = ...
    mask_min_space: int = ...
    require_same_masks: bool = ...
    mask_dropout: float = ...
    mask_channel_length: int = ...
    mask_channel_prob: float = ...
    mask_channel_before: bool = ...
    mask_channel_selection: MASKING_DISTRIBUTION_CHOICES = ...
    mask_channel_other: float = ...
    no_mask_channel_overlap: bool = ...
    mask_channel_min_space: int = ...
    num_negatives: int = ...
    negatives_from_everywhere: bool = ...
    cross_sample_negatives: int = ...
    codebook_negatives: int = ...
    conv_pos: int = ...
    conv_pos_groups: int = ...
    pos_conv_depth: int = ...
    latent_temp: Tuple[float, float, float] = ...
    max_positions: int = ...
    checkpoint_activations: bool = ...
    required_seq_len_multiple: int = ...
    crop_seq_to_multiple: int = ...
    depthwise_conv_kernel_size: int = ...
    attn_type: str = ...
    pos_enc_type: str = ...
    fp16: bool = ...


@register_model("wav2vec2", dataclass=Wav2Vec2Config)
class Wav2Vec2Model(BaseFairseqModel):
    def __init__(self, cfg: Wav2Vec2Config) -> None:
        ...
    
    def upgrade_state_dict_named(self, state_dict, name):
        ...
    
    @classmethod
    def build_model(cls, cfg: Wav2Vec2Config, task=...): # -> Self:
        """Build a new model instance."""
        ...
    
    def apply_mask(self, x, padding_mask, mask_indices=..., mask_channel_indices=...): # -> tuple[Tensor | Any, Tensor | Any | None]:
        ...
    
    def sample_negatives(self, y, num, padding_count=...): # -> tuple[Any, Tensor | Any]:
        ...
    
    def compute_preds(self, x, y, negatives): # -> Tensor:
        ...
    
    def forward(self, source, padding_mask=..., mask=..., features_only=..., layer=..., mask_indices=..., mask_channel_indices=..., padding_count=...): # -> dict[str, Any | Tensor | None] | dict[str, Tensor | Any | None]:
        ...
    
    def quantize(self, x): # -> tuple[Any, Any]:
        ...
    
    def extract_features(self, source, padding_mask, mask=..., layer=...): # -> dict[str, Any | Tensor | None] | dict[str, Tensor | Any | None]:
        ...
    
    def get_logits(self, net_output):
        ...
    
    def get_targets(self, sample, net_output, expand_steps=...):
        ...
    
    def get_extra_losses(self, net_output): # -> list[Any]:
        ...
    
    def remove_pretraining_modules(self, last_layer=...): # -> None:
        ...
    


class ConvFeatureExtractionModel(nn.Module):
    def __init__(self, conv_layers: List[Tuple[int, int, int]], dropout: float = ..., mode: str = ..., conv_bias: bool = ...) -> None:
        ...
    
    def forward(self, x): # -> Any:
        ...
    


def make_conv_pos(e, k, g): # -> Sequential:
    ...

class TransformerEncoder(nn.Module):
    def build_encoder_layer(self, args: Wav2Vec2Config):
        ...
    
    def __init__(self, args: Wav2Vec2Config) -> None:
        ...
    
    def forward(self, x, padding_mask=..., layer=...): # -> tuple[Any | Tensor, list[tuple[Any, Any, Any]]]:
        ...
    
    def extract_features(self, x, padding_mask=..., tgt_layer=..., min_layer=...): # -> tuple[Tensor | Any, list[tuple[Any, Any, Any]]]:
        ...
    
    def max_positions(self): # -> Tensor | Module:
        """Maximum output length supported by the encoder."""
        ...
    
    def upgrade_state_dict_named(self, state_dict, name):
        """Upgrade a (possibly old) state dict for new versions of fairseq."""
        ...
    


class ConformerEncoder(TransformerEncoder):
    def build_encoder_layer(self, args):
        ...
    
    def __init__(self, args) -> None:
        ...
    
    def extract_features(self, x, padding_mask=..., tgt_layer=...): # -> tuple[Tensor | Any, list[Any]]:
        ...
    


class TransformerSentenceEncoderLayer(nn.Module):
    """
    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained
    models.
    """
    def __init__(self, embedding_dim: float = ..., ffn_embedding_dim: float = ..., num_attention_heads: int = ..., dropout: float = ..., attention_dropout: float = ..., activation_dropout: float = ..., activation_fn: str = ..., layer_norm_first: bool = ...) -> None:
        ...
    
    def forward(self, x: torch.Tensor, self_attn_mask: torch.Tensor = ..., self_attn_padding_mask: torch.Tensor = ..., need_weights: bool = ..., att_args=...): # -> tuple[Tensor | Any, tuple[Any, Tensor]]:
        """
        LayerNorm is applied either before or after the self-attention/ffn
        modules similar to the original Transformer imlementation.
        """
        ...
    


