"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from typing import Dict, List, Optional, Tuple
from torch import Tensor
from fairseq.models import FairseqEncoder, FairseqEncoderDecoderModel, register_model, register_model_architecture

logger = ...
class Conv1dAdaptor(nn.Module):
    def __init__(self, in_dim, out_dim, n_layers=..., kernel_size=..., stride=..., layerdrop=..., layernorm=..., proj=...) -> None:
        ...
    
    @classmethod
    def add_args(cls, parser): # -> None:
        ...
    
    def forward(self, x, padding_mask: Optional[torch.Tensor]): # -> tuple[Any | Tensor, Any | None]:
        ...
    


def add_wav2vec_asr_args(parser): # -> None:
    ...

def need_finetuning(ft_params, param_name): # -> bool:
    ...

class Wav2VecEncoderWithAdaptor(FairseqEncoder):
    def build_adaptor(self, args): # -> Conv1dAdaptor | None:
        ...
    
    def __init__(self, args) -> None:
        ...
    
    @classmethod
    def add_args(cls, parser): # -> None:
        ...
    
    def set_num_updates(self, num_updates): # -> None:
        ...
    
    def forward(self, src_tokens, src_lengths=..., **kwargs): # -> dict[str, list[Any]]:
        ...
    
    def reorder_encoder_out(self, encoder_out, new_order): # -> dict[str, list[Any] | Any]:
        ...
    


def add_decoder_args(parser): # -> None:
    ...

def remove_weight_norm_from_model(model): # -> None:
    ...

@register_model("xm_transformer")
class XMTransformerModel(FairseqEncoderDecoderModel):
    @classmethod
    def hub_models(cls): # -> dict[str, str]:
        ...
    
    @classmethod
    def from_pretrained(cls, model_name_or_path, checkpoint_file=..., data_name_or_path=..., config_yaml=..., **kwargs): # -> S2THubInterface:
        ...
    
    def __init__(self, encoder, decoder) -> None:
        ...
    
    @classmethod
    def add_args(cls, parser): # -> None:
        """Add model-specific arguments to the parser."""
        ...
    
    @classmethod
    def maybe_load_pretrained(cls, component, checkpoint: Optional[str] = ...): # -> FairseqEncoder | FairseqDecoder:
        ...
    
    @classmethod
    def build_encoder(cls, args): # -> FairseqEncoder | FairseqDecoder:
        ...
    
    @classmethod
    def get_decoder_args_from_checkpoint(cls, ckpt_args): # -> dict[Any, Any]:
        ...
    
    @classmethod
    def override_decoder_args(cls, cli_args, decoder_args_dict):
        ...
    
    @classmethod
    def build_decoder(cls, args, task, embed_tokens): # -> FairseqEncoder | FairseqDecoder:
        ...
    
    @classmethod
    def build_model(cls, args, task): # -> Self:
        """Build a new model instance."""
        ...
    
    def get_normalized_probs(self, net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]], log_probs: bool, sample: Optional[Dict[str, Tensor]] = ...): # -> Any | Tensor:
        ...
    
    def forward(self, src_tokens, src_lengths, prev_output_tokens, **kwargs):
        """
        The forward method inherited from the base class has a **kwargs
        argument in its input, which is not supported in torchscript. This
        method overwrites the forward method definition without **kwargs.
        """
        ...
    
    def upgrade_state_dict(self, state_dict): # -> None:
        ...
    


def set_default_w2v_encoder_args(args): # -> None:
    ...

def set_default_adaptor_args(args): # -> None:
    ...

def set_default_transformer_decoder_args(args): # -> None:
    ...

def set_default_general_args(args): # -> None:
    ...

@register_model_architecture(model_name="xm_transformer", arch_name="xm_transformer")
def base_architecture(args): # -> None:
    ...

