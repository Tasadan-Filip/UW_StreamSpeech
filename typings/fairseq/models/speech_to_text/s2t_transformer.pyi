"""
This type stub file was generated by pyright.
"""

import torch.nn as nn
from typing import Dict, List, Optional, Tuple
from torch import Tensor
from fairseq.models import FairseqEncoder, FairseqEncoderDecoderModel, register_model, register_model_architecture
from fairseq.models.transformer import TransformerDecoder

logger = ...
class Conv1dSubsampler(nn.Module):
    """Convolutional subsampler: a stack of 1D convolution (along temporal
    dimension) followed by non-linear activation via gated linear units
    (https://arxiv.org/abs/1911.08460)

    Args:
        in_channels (int): the number of input channels
        mid_channels (int): the number of intermediate channels
        out_channels (int): the number of output channels
        kernel_sizes (List[int]): the kernel size for each convolutional layer
    """
    def __init__(self, in_channels: int, mid_channels: int, out_channels: int, kernel_sizes: List[int] = ...) -> None:
        ...
    
    def get_out_seq_lens_tensor(self, in_seq_lens_tensor):
        ...
    
    def forward(self, src_tokens, src_lengths): # -> tuple[Tensor | Any, Any]:
        ...
    


@register_model("s2t_transformer")
class S2TTransformerModel(FairseqEncoderDecoderModel):
    """Adapted Transformer model (https://arxiv.org/abs/1706.03762) for
    speech-to-text tasks. The Transformer encoder/decoder remains the same.
    A trainable input subsampler is prepended to the Transformer encoder to
    project inputs into the encoder dimension as well as downsample input
    sequence for computational efficiency."""
    @classmethod
    def hub_models(cls): # -> dict[str, str]:
        ...
    
    @classmethod
    def from_pretrained(cls, model_name_or_path, checkpoint_file=..., data_name_or_path=..., config_yaml=..., **kwargs): # -> S2THubInterface:
        ...
    
    def __init__(self, encoder, decoder) -> None:
        ...
    
    @staticmethod
    def add_args(parser): # -> None:
        """Add model-specific arguments to the parser."""
        ...
    
    @classmethod
    def build_encoder(cls, args): # -> S2TTransformerEncoder | FairseqEncoder | FairseqDecoder:
        ...
    
    @classmethod
    def build_decoder(cls, args, task, embed_tokens): # -> TransformerDecoderScriptable:
        ...
    
    @classmethod
    def build_model(cls, args, task): # -> Self:
        """Build a new model instance."""
        ...
    
    def get_normalized_probs(self, net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]], log_probs: bool, sample: Optional[Dict[str, Tensor]] = ...): # -> Any | Tensor:
        ...
    
    def get_ctc_target(self, sample: Optional[Dict[str, Tensor]]): # -> tuple[Tensor | Any, Tensor | Any]:
        ...
    
    def get_ctc_output(self, net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]], sample: Optional[Dict[str, Tensor]]): # -> tuple[Tensor, Any | Tensor]:
        ...
    
    def forward(self, src_tokens, src_lengths, prev_output_tokens):
        """
        The forward method inherited from the base class has a **kwargs
        argument in its input, which is not supported in torchscript. This
        method overwrites the forward method definition without **kwargs.
        """
        ...
    


class S2TTransformerEncoder(FairseqEncoder):
    """Speech-to-text Transformer encoder that consists of input subsampler and
    Transformer encoder."""
    def __init__(self, args) -> None:
        ...
    
    def forward(self, src_tokens, src_lengths, return_all_hiddens=...): # -> dict[str, list[Any]]:
        ...
    
    def reorder_encoder_out(self, encoder_out, new_order): # -> dict[str, list[Any] | Any]:
        ...
    
    def set_num_updates(self, num_updates): # -> None:
        ...
    


class TransformerDecoderScriptable(TransformerDecoder):
    def extract_features(self, prev_output_tokens, encoder_out: Optional[Dict[str, List[Tensor]]] = ..., incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = ..., full_context_alignment: bool = ..., alignment_layer: Optional[int] = ..., alignment_heads: Optional[int] = ...): # -> tuple[Any, dict[str, Dict[str, List[Tensor]] | None] | None]:
        ...
    


@register_model_architecture(model_name="s2t_transformer", arch_name="s2t_transformer")
def base_architecture(args): # -> None:
    ...

@register_model_architecture("s2t_transformer", "s2t_transformer_s")
def s2t_transformer_s(args): # -> None:
    ...

@register_model_architecture("s2t_transformer", "s2t_transformer_xs")
def s2t_transformer_xs(args): # -> None:
    ...

@register_model_architecture("s2t_transformer", "s2t_transformer_sp")
def s2t_transformer_sp(args): # -> None:
    ...

@register_model_architecture("s2t_transformer", "s2t_transformer_m")
def s2t_transformer_m(args): # -> None:
    ...

@register_model_architecture("s2t_transformer", "s2t_transformer_mp")
def s2t_transformer_mp(args): # -> None:
    ...

@register_model_architecture("s2t_transformer", "s2t_transformer_l")
def s2t_transformer_l(args): # -> None:
    ...

@register_model_architecture("s2t_transformer", "s2t_transformer_lp")
def s2t_transformer_lp(args): # -> None:
    ...

