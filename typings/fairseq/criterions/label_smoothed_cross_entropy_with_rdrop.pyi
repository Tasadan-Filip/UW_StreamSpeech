"""
This type stub file was generated by pyright.
"""

from dataclasses import dataclass
from fairseq.criterions import register_criterion
from fairseq.criterions.label_smoothed_cross_entropy import LabelSmoothedCrossEntropyCriterion, LabelSmoothedCrossEntropyCriterionConfig

@dataclass
class RdropLabelSmoothedCrossEntropyCriterionConfig(LabelSmoothedCrossEntropyCriterionConfig):
    rdrop_alpha: float = ...


@register_criterion("label_smoothed_cross_entropy_with_rdrop", dataclass=RdropLabelSmoothedCrossEntropyCriterionConfig)
class RdropLabelSmoothedCrossEntropyCriterion(LabelSmoothedCrossEntropyCriterion):
    def __init__(self, task, sentence_avg, label_smoothing, ignore_prefix_size=..., report_accuracy=..., rdrop_alpha=...) -> None:
        ...
    
    def forward(self, model, sample, reduce=..., net_output=...): # -> tuple[Any, Any, dict[str, Any]]:
        """Compute the loss for the given sample.

        Returns a tuple with three elements:
        1) the loss
        2) the sample size, which is used as the denominator for the gradient
        3) logging outputs to display while training
        """
        ...
    
    def get_lprobs_and_target(self, model, net_output, sample): # -> tuple[Any, Tensor | Any]:
        ...
    
    def compute_loss(self, model, net_output, sample, reduce=...): # -> tuple[Any, Any, Tensor | Any]:
        ...
    
    @classmethod
    def reduce_metrics(cls, logging_outputs) -> None:
        """Aggregate logging outputs from data parallel training."""
        ...
    


def duplicate_input(sample):
    ...

def compute_kl_loss(model, net_output, pad_mask=..., reduce=...): # -> Tensor:
    ...

