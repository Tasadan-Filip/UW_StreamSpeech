"""
This type stub file was generated by pyright.
"""

from typing import Any, Dict, List
from fairseq.dataclass import FairseqDataclass
from torch.nn.modules.loss import _Loss

class FairseqCriterion(_Loss):
    def __init__(self, task) -> None:
        ...
    
    @classmethod
    def add_args(cls, parser): # -> None:
        """Add criterion-specific arguments to the parser."""
        ...
    
    @classmethod
    def build_criterion(cls, cfg: FairseqDataclass, task): # -> Self:
        """Construct a criterion from command-line args."""
        ...
    
    def forward(self, model, sample, reduce=...):
        """Compute the loss for the given sample.

        Returns a tuple with three elements:
        1) the loss
        2) the sample size, which is used as the denominator for the gradient
        3) logging outputs to display while training
        """
        ...
    
    @staticmethod
    def aggregate_logging_outputs(logging_outputs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Aggregate logging outputs from data parallel training."""
        ...
    
    @classmethod
    def reduce_metrics(cls, logging_outputs: List[Dict[str, Any]]) -> None:
        """Aggregate logging outputs from data parallel training."""
        ...
    
    @staticmethod
    def logging_outputs_can_be_summed() -> bool:
        """
        Whether the logging outputs returned by `forward` can be summed
        across workers prior to calling `reduce_metrics`. Setting this
        to True will improves distributed training speed.
        """
        ...
    


class LegacyFairseqCriterion(FairseqCriterion):
    def __init__(self, args, task) -> None:
        ...
    
    @classmethod
    def build_criterion(cls, args, task): # -> Self:
        """Construct a criterion from command-line args."""
        ...
    


