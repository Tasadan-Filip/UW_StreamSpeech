"""
This type stub file was generated by pyright.
"""

from dataclasses import dataclass
from fairseq.criterions import FairseqCriterion, register_criterion
from fairseq.dataclass import FairseqDataclass

@dataclass
class LabelSmoothedCrossEntropyCriterionConfig(FairseqDataclass):
    label_smoothing: float = ...
    report_accuracy: bool = ...
    ignore_prefix_size: int = ...
    sentence_avg: bool = ...


def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=..., reduce=...): # -> tuple[Any, Any]:
    ...

@register_criterion("label_smoothed_cross_entropy", dataclass=LabelSmoothedCrossEntropyCriterionConfig)
class LabelSmoothedCrossEntropyCriterion(FairseqCriterion):
    def __init__(self, task, sentence_avg, label_smoothing, ignore_prefix_size=..., report_accuracy=...) -> None:
        ...
    
    def forward(self, model, sample, reduce=...): # -> tuple[Any, Any, dict[str, Any]]:
        """Compute the loss for the given sample.

        Returns a tuple with three elements:
        1) the loss
        2) the sample size, which is used as the denominator for the gradient
        3) logging outputs to display while training
        """
        ...
    
    def get_lprobs_and_target(self, model, net_output, sample): # -> tuple[Any, Any]:
        ...
    
    def compute_loss(self, model, net_output, sample, reduce=...): # -> tuple[Any, Any]:
        ...
    
    def compute_accuracy(self, model, net_output, sample): # -> tuple[Tensor, Tensor]:
        ...
    
    @classmethod
    def reduce_metrics(cls, logging_outputs) -> None:
        """Aggregate logging outputs from data parallel training."""
        ...
    
    @staticmethod
    def logging_outputs_can_be_summed() -> bool:
        """
        Whether the logging outputs returned by `forward` can be summed
        across workers prior to calling `reduce_metrics`. Setting this
        to True will improves distributed training speed.
        """
        ...
    


