"""
This type stub file was generated by pyright.
"""

from typing import Callable, Dict, List
from . import FairseqDataset

def uniform_sampler(x): # -> int:
    ...

class MultiCorpusSampledDataset(FairseqDataset):
    """
    Stores multiple instances of FairseqDataset together and in every iteration
    creates a batch by first sampling a dataset according to a specified
    probability distribution and then getting instances from that dataset.

    Args:
        datasets: an OrderedDict of FairseqDataset instances.
        sampling_func: A function for sampling over list of dataset keys.
            The default strategy is to sample uniformly.
    """
    def __init__(self, datasets: Dict[str, FairseqDataset], sampling_func: Callable[[List], int] = ...) -> None:
        ...
    
    def __len__(self): # -> int:
        """
        Length of this dataset is the sum of individual datasets
        """
        ...
    
    def ordered_indices(self): # -> NDArray[signedinteger[Any]]:
        """
        Ordered indices for batching. Here we call the underlying
        dataset's ordered_indices() so that we get the same random ordering
        as we would have from using the underlying dataset directly.
        """
        ...
    
    def __getitem__(self, index: int): # -> OrderedDict[str, Any]:
        """
        Get the item associated with index from each underlying dataset.
        Since index is in the range of [0, TotalNumInstances], we need to
        map the index to the dataset before retrieving the item.
        """
        ...
    
    def collater(self, samples: List[Dict]): # -> None:
        """
        Generate a mini-batch for this dataset.
        To convert this into a regular mini-batch we use the following
        logic:
            1. Select a dataset using the specified probability distribution.
            2. Call the collater function of the selected dataset.
        """
        ...
    
    def num_tokens(self, index: int):
        """
        Return an example's length (number of tokens), used for batching. Here
        we return the max across all examples at index across all underlying
        datasets.
        """
        ...
    
    def size(self, index: int):
        """
        Return an example's size as a float or tuple. Here we return the max
        across all underlying datasets. This value is used when filtering a
        dataset with max-positions.
        """
        ...
    
    @property
    def supports_prefetch(self): # -> bool:
        ...
    
    def prefetch(self, indices): # -> None:
        ...
    
    @property
    def supports_fetch_outside_dataloader(self): # -> bool:
        ...
    


