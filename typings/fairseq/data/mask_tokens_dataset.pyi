"""
This type stub file was generated by pyright.
"""

import torch
from functools import lru_cache
from fairseq.data import Dictionary
from . import BaseWrapperDataset

class MaskTokensDataset(BaseWrapperDataset):
    """
    A wrapper Dataset for masked language modeling.

    Input items are masked according to the specified masking probability.

    Args:
        dataset: Dataset to wrap.
        sizes: Sentence lengths
        vocab: Dictionary with the vocabulary and special tokens.
        pad_idx: Id of pad token in vocab
        mask_idx: Id of mask token in vocab
        return_masked_tokens: controls whether to return the non-masked tokens
            (the default) or to return a tensor with the original masked token
            IDs (and *pad_idx* elsewhere). The latter is useful as targets for
            masked LM training.
        seed: Seed for random number generator for reproducibility.
        mask_prob: probability of replacing a token with *mask_idx*.
        leave_unmasked_prob: probability that a masked token is unmasked.
        random_token_prob: probability of replacing a masked token with a
            random token from the vocabulary.
        freq_weighted_replacement: sample random replacement words based on
            word frequencies in the vocab.
        mask_whole_words: only mask whole words. This should be a byte mask
            over vocab indices, indicating whether it is the beginning of a
            word. We will extend any mask to encompass the whole word.
        bpe: BPE to use for whole-word masking.
        mask_multiple_length : repeat each mask index multiple times. Default
            value is 1.
        mask_stdev : standard deviation of masks distribution in case of
            multiple masking. Default value is 0.
    """
    @classmethod
    def apply_mask(cls, dataset: torch.utils.data.Dataset, *args, **kwargs): # -> tuple[LRUCacheDataset, LRUCacheDataset]:
        """Return the source and target datasets for masked LM training."""
        ...
    
    def __init__(self, dataset: torch.utils.data.Dataset, vocab: Dictionary, pad_idx: int, mask_idx: int, return_masked_tokens: bool = ..., seed: int = ..., mask_prob: float = ..., leave_unmasked_prob: float = ..., random_token_prob: float = ..., freq_weighted_replacement: bool = ..., mask_whole_words: torch.Tensor = ..., mask_multiple_length: int = ..., mask_stdev: float = ...) -> None:
        ...
    
    @property
    def can_reuse_epoch_itr_across_epochs(self): # -> Literal[True]:
        ...
    
    def set_epoch(self, epoch, **unused): # -> None:
        ...
    
    def __getitem__(self, index: int): # -> Tensor:
        ...
    
    @lru_cache(maxsize=8)
    def __getitem_cached__(self, seed: int, epoch: int, index: int): # -> Tensor:
        ...
    


