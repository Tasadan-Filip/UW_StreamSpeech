"""
This type stub file was generated by pyright.
"""

import contextlib
import numpy as np
import torch
from typing import Optional, Tuple

logger = ...
def infer_language_pair(path): # -> list[str] | tuple[None, None]:
    """Infer language pair from filename: <split>.<lang1>-<lang2>.(...).idx"""
    ...

def collate_tokens(values, pad_idx, eos_idx=..., left_pad=..., move_eos_to_beginning=..., pad_to_length=..., pad_to_multiple=..., pad_to_bsz=...):
    """Convert a list of 1d tensors into a padded 2d tensor."""
    ...

def load_indexed_dataset(path, dictionary=..., dataset_impl=..., combine=..., default=...): # -> ConcatDataset | None:
    """A helper function for loading indexed datasets.

    Args:
        path (str): path to indexed dataset (e.g., 'data-bin/train')
        dictionary (~fairseq.data.Dictionary): data dictionary
        dataset_impl (str, optional): which dataset implementation to use. If
            not provided, it will be inferred automatically. For legacy indexed
            data we use the 'cached' implementation by default.
        combine (bool, optional): automatically load and combine multiple
            datasets. For example, if *path* is 'data-bin/train', then we will
            combine 'data-bin/train', 'data-bin/train1', ... and return a
            single ConcatDataset instance.
    """
    ...

@contextlib.contextmanager
def numpy_seed(seed, *addl_seeds): # -> Generator[None, Any, None]:
    """Context manager which seeds the NumPy PRNG with the specified seed and
    restores the state afterward"""
    ...

def collect_filtered(function, iterable, filtered): # -> Generator[Any, Any, None]:
    """
    Similar to :func:`filter` but collects filtered elements in ``filtered``.

    Args:
        function (callable): function that returns ``False`` for elements that
            should be filtered
        iterable (iterable): iterable to filter
        filtered (list): list to store filtered elements
    """
    ...

def filter_by_size(indices, dataset, max_positions, raise_exception=...): # -> NDArray[signedinteger[_64Bit]]:
    """
    [deprecated] Filter indices based on their size.
    Use `FairseqDataset::filter_indices_by_size` instead.

    Args:
        indices (List[int]): ordered list of dataset indices
        dataset (FairseqDataset): fairseq dataset instance
        max_positions (tuple): filter elements larger than this size.
            Comparisons are done component-wise.
        raise_exception (bool, optional): if ``True``, raise an exception if
            any elements are filtered (default: False).
    """
    ...

def filter_paired_dataset_indices_by_size(src_sizes, tgt_sizes, indices, max_sizes): # -> tuple[Any, list[Any]] | tuple[Any, Any]:
    """Filter a list of sample indices. Remove those that are longer
        than specified in max_sizes.

    Args:
        indices (np.array): original array of sample indices
        max_sizes (int or list[int] or tuple[int]): max sample size,
            can be defined separately for src and tgt (then list or tuple)

    Returns:
        np.array: filtered sample array
        list: list of removed indices
    """
    ...

def batch_by_size(indices, num_tokens_fn, num_tokens_vec=..., max_tokens=..., max_sentences=..., required_batch_size_multiple=..., fixed_shapes=...):
    """
    Yield mini-batches of indices bucketed by size. Batches may contain
    sequences of different lengths.

    Args:
        indices (List[int]): ordered list of dataset indices
        num_tokens_fn (callable): function that returns the number of tokens at
            a given index
        num_tokens_vec (List[int], optional): precomputed vector of the number
            of tokens for each index in indices (to enable faster batch generation)
        max_tokens (int, optional): max number of tokens in each batch
            (default: None).
        max_sentences (int, optional): max number of sentences in each
            batch (default: None).
        required_batch_size_multiple (int, optional): require batch size to
            be less than N or a multiple of N (default: 1).
        fixed_shapes (List[Tuple[int, int]], optional): if given, batches will
            only be created with the given shapes. *max_sentences* and
            *required_batch_size_multiple* will be ignored (default: None).
    """
    ...

def post_process(sentence: str, symbol: str): # -> str:
    ...

def compute_mask_indices(shape: Tuple[int, int], padding_mask: Optional[torch.Tensor], mask_prob: float, mask_length: int, mask_type: str = ..., mask_other: float = ..., min_masks: int = ..., no_overlap: bool = ..., min_space: int = ..., require_same_masks: bool = ..., mask_dropout: float = ...) -> np.ndarray:
    """
    Computes random mask spans for a given shape

    Args:
        shape: the the shape for which to compute masks.
            should be of size 2 where first element is batch size and 2nd is timesteps
        padding_mask: optional padding mask of the same size as shape, which will prevent masking padded elements
        mask_prob: probability for each token to be chosen as start of the span to be masked. this will be multiplied by
            number of timesteps divided by length of mask span to mask approximately this percentage of all elements.
            however due to overlaps, the actual number will be smaller (unless no_overlap is True)
        mask_type: how to compute mask lengths
            static = fixed size
            uniform = sample from uniform distribution [mask_other, mask_length*2]
            normal = sample from normal distribution with mean mask_length and stdev mask_other. mask is min 1 element
            poisson = sample from possion distribution with lambda = mask length
        min_masks: minimum number of masked spans
        no_overlap: if false, will switch to an alternative recursive algorithm that prevents spans from overlapping
        min_space: only used if no_overlap is True, this is how many elements to keep unmasked between spans
        require_same_masks: if true, will randomly drop out masks until same amount of masks remains in each sample
        mask_dropout: randomly dropout this percentage of masks in each example
    """
    ...

def get_mem_usage(): # -> str:
    ...

def lengths_to_padding_mask(lens):
    ...

def lengths_to_mask(lens):
    ...

def get_buckets(sizes, num_buckets):
    ...

def get_bucketed_sizes(orig_sizes, buckets): # -> NDArray[Any]:
    ...

def raise_if_valid_subsets_unintentionally_ignored(train_cfg) -> None:
    """Raises if there are paths matching 'valid*[0-9].*' which are not combined or ignored."""
    ...

