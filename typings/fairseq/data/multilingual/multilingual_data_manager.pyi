"""
This type stub file was generated by pyright.
"""

logger = ...
SRC_DICT_NAME = ...
TGT_DICT_NAME = ...
def load_sampling_weights(from_file): # -> Any:
    ...

class MultilingualDatasetManager:
    def __init__(self, args, lang_pairs, langs, dicts, sampling_method) -> None:
        ...
    
    @classmethod
    def setup_data_manager(cls, args, lang_pairs, langs, dicts, sampling_method): # -> MultilingualDatasetManager:
        ...
    
    @staticmethod
    def add_args(parser): # -> None:
        ...
    
    @classmethod
    def load_langs(cls, args, **kwargs): # -> list[Any] | list[str]:
        ...
    
    def has_sharded_data(self, split): # -> Literal[False]:
        ...
    
    def estimate_global_pass_epoch(self, epoch): # -> None:
        ...
    
    @classmethod
    def prepare(cls, load_dictionary, args, **kargs): # -> tuple[list[Any] | list[str] | Any, dict[Any, Any] | OrderedDict[Any, Any], bool]:
        ...
    
    @classmethod
    def load_all_dictionaries(cls, args, language_list, load_dictionary, training): # -> dict[Any, Any] | OrderedDict[Any, Any]:
        ...
    
    def get_source_dictionary(self, lang):
        ...
    
    def get_target_dictionary(self, lang):
        ...
    
    @classmethod
    def create_lang_dictionary(cls, langs): # -> Dictionary:
        ...
    
    @classmethod
    def get_langtok_index(cls, lang_tok, dic):
        ...
    
    def get_encoder_langtok(self, src_lang, tgt_lang, spec=...): # -> None:
        ...
    
    def get_decoder_langtok(self, tgt_lang, spec=...): # -> None:
        ...
    
    @classmethod
    def load_data(cls, path, vdict, impl): # -> ConcatDataset | None:
        ...
    
    @classmethod
    def split_exists(cls, split, src, tgt, lang, data_path, dataset_impl): # -> bool:
        ...
    
    def load_lang_dataset(self, data_path, split, src, src_dict, tgt, tgt_dict, combine, dataset_impl, upsample_primary, max_source_positions, prepend_bos=..., load_alignments=..., truncate_source=...): # -> tuple[PrependTokenDataset | Any | ConcatDataset, PrependTokenDataset | Any | ConcatDataset, Any | ConcatDataset | None]:
        ...
    
    def load_langpair_dataset(self, data_path, split, src, src_dict, tgt, tgt_dict, combine, dataset_impl, upsample_primary, left_pad_source, left_pad_target, max_source_positions, max_target_positions, prepend_bos=..., load_alignments=..., truncate_source=..., src_dataset_transform_func=..., tgt_dataset_transform_func=..., src_lang_id=..., tgt_lang_id=..., langpairs_sharing_datasets=...): # -> LanguagePairDataset:
        ...
    
    def src_dataset_tranform_func(self, src_lang, tgt_lang, dataset, spec=...): # -> PrependTokenDataset:
        ...
    
    def tgt_dataset_tranform_func(self, source_lang, target_lang, dataset, spec=...): # -> PrependTokenDataset | None:
        ...
    
    def alter_dataset_langtok(self, lang_pair_dataset, src_eos=..., src_lang=..., tgt_eos=..., tgt_lang=..., src_langtok_spec=..., tgt_langtok_spec=...): # -> TransformEosLangPairDataset:
        ...
    
    def load_a_dataset(self, split, data_path, src, src_dict, tgt, tgt_dict, combine, prepend_bos=..., langpairs_sharing_datasets=..., data_category=..., **extra_kwargs): # -> TransformEosLangPairDataset | LanguagePairDataset:
        ...
    
    def load_split_langpair_datasets(self, split, data_param_list): # -> list[Any]:
        ...
    
    def get_data_paths_and_lang_pairs(self, split): # -> tuple[dict[str, Any], dict[str, Any]]:
        ...
    
    @classmethod
    def get_dataset_key(cls, data_category, src, tgt): # -> str:
        ...
    
    def get_split_num_data_shards(self, split): # -> dict[Any, Any]:
        ...
    
    @classmethod
    def get_shard_id(cls, num_shards, epoch, shard_epoch=...):
        ...
    
    def get_split_data_path(self, paths, epoch, shard_epoch, num_shards):
        ...
    
    def get_split_data_param_list(self, split, epoch, shard_epoch=...): # -> list[Any]:
        ...
    
    def get_train_dataset_sizes(self, data_param_list, datasets, epoch, shard_epoch=...): # -> list[Any]:
        ...
    
    def get_train_sampling_ratios(self, data_param_list, datasets, epoch=..., shard_epoch=...): # -> None:
        ...
    
    def get_sampling_ratios(self, data_param_list, datasets, epoch, shard_epoch=...): # -> list[Any] | None:
        ...
    
    def load_split_datasets(self, split, training, epoch=..., combine=..., shard_epoch=..., **kwargs): # -> tuple[list[tuple[Any, Any | TransformEosLangPairDataset | LanguagePairDataset]], list[Any]]:
        ...
    
    def load_into_concat_dataset(self, split, datasets, data_param_list): # -> SampledMultiDataset | ConcatDataset:
        ...
    
    def load_sampled_multi_epoch_dataset(self, split, training, epoch=..., combine=..., shard_epoch=..., **kwargs): # -> SampledMultiEpochDataset | SampledMultiDataset | ConcatDataset:
        ...
    
    def load_sampled_multi_dataset(self, split, training, epoch=..., combine=..., shard_epoch=..., **kwargs): # -> SampledMultiDataset | ConcatDataset:
        ...
    
    def load_dataset(self, split, training, epoch=..., combine=..., shard_epoch=..., **kwargs): # -> SampledMultiDataset | ConcatDataset | SampledMultiEpochDataset:
        ...
    


