"""
This type stub file was generated by pyright.
"""

from . import FairseqDataset

def collate(samples, pad_idx, eos_idx, vocab, left_pad_source=..., left_pad_target=..., input_feeding=..., pad_to_length=...): # -> dict[Any, Any] | dict[str, Tensor | int | dict[str, Any] | Any | None]:
    ...

class DenoisingDataset(FairseqDataset):
    """
    A wrapper around TokenBlockDataset for BART dataset.

    Args:
        dataset (TokenBlockDataset): dataset to wrap
        sizes (List[int]): sentence lengths
        vocab (~fairseq.data.Dictionary): vocabulary
        mask_idx (int): dictionary index used for masked token
        mask_whole_words: only mask whole words. This should be a byte mask
            over vocab indices, indicating whether it is the beginning of a
            word. We will extend any mask to encompass the whole word.
        shuffle (bool, optional): shuffle the elements before batching.
          Default: ``True``
        seed: Seed for random number generator for reproducibility.
    """
    def __init__(self, dataset, sizes, vocab, mask_idx, mask_whole_words, shuffle, seed, mask, mask_random, insert, rotate, permute_sentences, bpe, replace_length, mask_length, poisson_lambda, eos=..., item_transform_func=...) -> None:
        ...
    
    @property
    def can_reuse_epoch_itr_across_epochs(self): # -> Literal[True]:
        ...
    
    def set_epoch(self, epoch, **unused): # -> None:
        ...
    
    def __getitem__(self, index): # -> dict[str, Any | Tensor]:
        ...
    
    def __len__(self): # -> int:
        ...
    
    def permute_sentences(self, source, p=...):
        ...
    
    def word_starts(self, source): # -> Tensor:
        ...
    
    def add_whole_word_mask(self, source, p): # -> Tensor:
        ...
    
    def add_permuted_noise(self, tokens, p):
        ...
    
    def add_rolling_noise(self, tokens): # -> Tensor:
        ...
    
    def add_insertion_noise(self, tokens, p): # -> Tensor:
        ...
    
    def collater(self, samples, pad_to_length=...): # -> dict[Any, Any] | dict[str, Tensor | int | dict[str, Any] | Any | None]:
        """Merge a list of samples to form a mini-batch.
        Args:
            samples (List[dict]): samples to collate
        Returns:
            dict: a mini-batch of data
        """
        ...
    
    def num_tokens(self, index):
        """Return the number of tokens in a sample. This value is used to
        enforce ``--max-tokens`` during batching."""
        ...
    
    def size(self, index):
        """Return an example's size as a float or tuple. This value is used when
        filtering a dataset with ``--max-positions``."""
        ...
    
    def ordered_indices(self): # -> ndarray[Any, dtype[int_]]:
        """Return an ordered list of indices. Batches will be constructed based
        on this order."""
        ...
    
    def prefetch(self, indices): # -> None:
        ...
    
    @property
    def supports_prefetch(self): # -> Literal[False]:
        ...
    


