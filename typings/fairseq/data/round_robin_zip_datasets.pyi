"""
This type stub file was generated by pyright.
"""

from . import FairseqDataset

logger = ...
class RoundRobinZipDatasets(FairseqDataset):
    """Zip multiple :class:`~fairseq.data.FairseqDataset` instances together.

    Shorter datasets are repeated in a round-robin fashion to match the length
    of the longest one.

    Args:
        datasets (Dict[~fairseq.data.FairseqDataset]): a dictionary of
            :class:`~fairseq.data.FairseqDataset` instances.
        eval_key (str, optional): a key used at evaluation time that causes
            this instance to pass-through batches from *datasets[eval_key]*.
    """
    def __init__(self, datasets, eval_key=...) -> None:
        ...
    
    def __getitem__(self, index): # -> OrderedDict[Any, Any]:
        ...
    
    def __len__(self): # -> int:
        ...
    
    def collater(self, samples): # -> OrderedDict[Any, Any] | None:
        """Merge a list of samples to form a mini-batch."""
        ...
    
    def num_tokens(self, index):
        """Return an example's length (number of tokens), used for batching."""
        ...
    
    def size(self, index): # -> dict[Any, Any]:
        """Return an example's size as a float or tuple. This value is used when
        filtering a dataset with ``--max-positions``."""
        ...
    
    def ordered_indices(self): # -> NDArray[signedinteger[Any]]:
        """Ordered indices for batching."""
        ...
    
    def filter_indices_by_size(self, indices, max_positions=...): # -> tuple[NDArray[signedinteger[Any]], list[int]]:
        """
        Filter each sub-dataset independently, then update the round robin to work
        on the filtered sub-datasets.
        """
        ...
    
    @property
    def supports_prefetch(self): # -> bool:
        ...
    
    def prefetch(self, indices): # -> None:
        ...
    


