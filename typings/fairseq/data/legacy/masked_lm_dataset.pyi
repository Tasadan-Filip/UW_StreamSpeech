"""
This type stub file was generated by pyright.
"""

import numpy as np
from typing import Dict, List
from fairseq.data import Dictionary, FairseqDataset

class MaskedLMDataset(FairseqDataset):
    """
    A wrapper Dataset for masked language modelling. The dataset
    wraps around TokenBlockDataset or BlockedPairDataset and creates a batch
    where the input blocks are masked according to the specified masking
    probability. Additionally the batch can also contain sentence level targets
    if this is specified.

    Args:
        dataset: Dataset which generates blocks of data. Only BlockPairDataset
            and TokenBlockDataset are supported.
        sizes: Sentence lengths
        vocab: Dictionary with the vocabulary and special tokens.
        pad_idx: Id of padding token in dictionary
        mask_idx: Id of mask token in dictionary
        classif_token_idx: Id of classification token in dictionary. This is the
            token associated with the sentence embedding (Eg: CLS for BERT)
        sep_token_idx: Id of separator token in dictionary
            (Eg: SEP in BERT)
        seed: Seed for random number generator for reproducibility.
        shuffle: Shuffle the elements before batching.
        has_pairs: Specifies whether the underlying dataset
            generates a pair of blocks along with a sentence_target or not.
            Setting it to True assumes that the underlying dataset generates a
            label for the pair of sentences which is surfaced as
            sentence_target. The default value assumes a single block with no
            sentence target.
        segment_id: An optional segment id for filling in the segment labels
            when we are in the single block setting (Eg: XLM). Default is 0.
        masking_ratio: specifies what percentage of the blocks should be masked.
        masking_prob: specifies the probability of a given token being
            replaced with the "MASK" token.
        random_token_prob: specifies the probability of a given token being
            replaced by a random token from the vocabulary.
    """
    def __init__(self, dataset: FairseqDataset, sizes: np.ndarray, vocab: Dictionary, pad_idx: int, mask_idx: int, classif_token_idx: int, sep_token_idx: int, seed: int = ..., shuffle: bool = ..., has_pairs: bool = ..., segment_id: int = ..., masking_ratio: float = ..., masking_prob: float = ..., random_token_prob: float = ...) -> None:
        ...
    
    def __getitem__(self, index: int): # -> dict[str, int | Tensor | Any | tuple[Tensor, Tensor, Tensor] | tuple[Tensor, Tensor, Any] | None]:
        ...
    
    def __len__(self): # -> int:
        ...
    
    def collater(self, samples: List[Dict]): # -> dict[Any, Any] | dict[str, LongTensor | int | dict[str, Any] | Any | None]:
        """Merge a list of samples to form a mini-batch.

        Args:
            samples (List[dict]): samples to collate

        Returns:
            dict: a mini-batch of data
        """
        ...
    
    def num_tokens(self, index: int): # -> Any:
        """
        Return the number of tokens in a sample. This value is used to
        enforce max-tokens during batching.
        """
        ...
    
    def size(self, index: int): # -> Any:
        """
        Return an example's size as a float or tuple. This value is used when
        filtering a dataset with max-positions.
        """
        ...
    
    def ordered_indices(self): # -> ndarray[Any, dtype[int_]] | Any:
        """
        Return an ordered list of indices. Batches will be constructed based
        on this order.
        """
        ...
    
    @property
    def supports_prefetch(self): # -> Any | bool:
        ...
    
    def prefetch(self, indices): # -> None:
        ...
    


