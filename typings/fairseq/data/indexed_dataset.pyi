"""
This type stub file was generated by pyright.
"""

import numpy as np
import torch
from functools import lru_cache
from . import FairseqDataset
from typing import Union

def best_fitting_int_dtype(max_int_to_represent) -> Union[np.uint16, np.uint32, np.int64]:
    ...

def get_available_dataset_impl(): # -> list[str]:
    ...

def infer_dataset_impl(path): # -> Literal['raw', 'cached', 'mmap', 'huffman', 'fasta'] | None:
    ...

def make_builder(out_file, impl, vocab_size=...): # -> MMapIndexedDatasetBuilder | IndexedDatasetBuilder:
    ...

def make_dataset(path, impl, fix_lua_indexing=..., dictionary=...): # -> IndexedRawTextDataset | IndexedDataset | IndexedCachedDataset | MMapIndexedDataset | EncodedFastaDataset | HuffmanMMapIndexedDataset | None:
    ...

def dataset_exists(path, impl): # -> bool:
    ...

def read_longs(f, n): # -> NDArray[signedinteger[_64Bit]]:
    ...

def write_longs(f, a): # -> None:
    ...

_code_to_dtype = ...
def index_file_path(prefix_path):
    ...

def data_file_path(prefix_path):
    ...

class IndexedDataset(FairseqDataset):
    """Loader for TorchNet IndexedDataset"""
    _HDR_MAGIC = ...
    def __init__(self, path, fix_lua_indexing=...) -> None:
        ...
    
    def read_index(self, path): # -> None:
        ...
    
    def read_data(self, path): # -> None:
        ...
    
    def check_index(self, i): # -> None:
        ...
    
    def __del__(self): # -> None:
        ...
    
    @lru_cache(maxsize=8)
    def __getitem__(self, i) -> torch.Tensor:
        ...
    
    def __len__(self): # -> Any:
        ...
    
    def num_tokens(self, index): # -> ndarray[Any, dtype[signedinteger[_64Bit]]]:
        ...
    
    def size(self, index): # -> ndarray[Any, dtype[signedinteger[_64Bit]]]:
        ...
    
    @staticmethod
    def exists(path): # -> bool:
        ...
    
    @property
    def supports_prefetch(self): # -> Literal[False]:
        ...
    


class IndexedCachedDataset(IndexedDataset):
    def __init__(self, path, fix_lua_indexing=...) -> None:
        ...
    
    @property
    def supports_prefetch(self): # -> Literal[True]:
        ...
    
    def prefetch(self, indices): # -> None:
        ...
    
    @lru_cache(maxsize=8)
    def __getitem__(self, i): # -> Tensor:
        ...
    


class IndexedRawTextDataset(FairseqDataset):
    """Takes a text file as input and binarizes it in memory at instantiation.
    Original lines are also kept in memory"""
    def __init__(self, path, dictionary, append_eos=..., reverse_order=...) -> None:
        ...
    
    def read_data(self, path, dictionary): # -> None:
        ...
    
    def check_index(self, i): # -> None:
        ...
    
    @lru_cache(maxsize=8)
    def __getitem__(self, i):
        ...
    
    def get_original_text(self, i):
        ...
    
    def __del__(self): # -> None:
        ...
    
    def __len__(self): # -> Callable[..., ndarray[Any, dtype[Any]] | Any]:
        ...
    
    def num_tokens(self, index): # -> ndarray[Any, dtype[Any]]:
        ...
    
    def size(self, index): # -> ndarray[Any, dtype[Any]]:
        ...
    
    @staticmethod
    def exists(path): # -> bool:
        ...
    


class IndexedDatasetBuilder:
    element_sizes = ...
    def __init__(self, out_file, dtype=...) -> None:
        ...
    
    def add_item(self, tensor): # -> None:
        ...
    
    def merge_file_(self, another_file): # -> None:
        ...
    
    def finalize(self, index_file): # -> None:
        ...
    


class MMapIndexedDataset(torch.utils.data.Dataset):
    class Index:
        _HDR_MAGIC = ...
        @classmethod
        def writer(cls, path, dtype): # -> _Writer:
            class _Writer:
                ...
            
            
        
        def __init__(self, path) -> None:
            ...
        
        def __del__(self): # -> None:
            ...
        
        @property
        def dtype(self): # -> uint8 | int8 | int16 | int32 | int64 | float64 | double | uint16 | uint32 | uint64:
            ...
        
        @property
        def sizes(self): # -> NDArray[signedinteger[_32Bit]]:
            ...
        
        @lru_cache(maxsize=8)
        def __getitem__(self, i): # -> tuple[ndarray[Any, dtype[signedinteger[_64Bit]]], ndarray[Any, dtype[signedinteger[_32Bit]]]]:
            ...
        
        def __len__(self): # -> Any:
            ...
        
    
    
    def __init__(self, path) -> None:
        ...
    
    def __getstate__(self): # -> None:
        ...
    
    def __setstate__(self, state): # -> None:
        ...
    
    def __del__(self): # -> None:
        ...
    
    def __len__(self): # -> int:
        ...
    
    @lru_cache(maxsize=8)
    def __getitem__(self, i): # -> Tensor:
        ...
    
    @property
    def sizes(self): # -> NDArray[signedinteger[_32Bit]]:
        ...
    
    @property
    def supports_prefetch(self): # -> Literal[False]:
        ...
    
    @staticmethod
    def exists(path): # -> bool:
        ...
    


def get_indexed_dataset_to_local(path) -> str:
    ...

class MMapIndexedDatasetBuilder:
    def __init__(self, out_file, dtype=...) -> None:
        ...
    
    def add_item(self, tensor): # -> None:
        ...
    
    def merge_file_(self, another_file): # -> None:
        ...
    
    def finalize(self, index_file): # -> None:
        ...
    


