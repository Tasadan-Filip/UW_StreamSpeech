"""
This type stub file was generated by pyright.
"""

from fairseq import optim
from omegaconf import DictConfig

logger = ...
class AMPOptimizer(optim.FairseqOptimizer):
    """
    Wrap an *optimizer* to support AMP (automatic mixed precision) training.
    """
    def __init__(self, cfg: DictConfig, params, fp32_optimizer, **kwargs) -> None:
        ...
    
    @classmethod
    def build_optimizer(cls, cfg: DictConfig, params, **kwargs): # -> Self:
        """
        Args:
            cfg (omegaconf.DictConfig): fairseq args
            params (iterable): iterable of parameters to optimize
        """
        ...
    
    def backward(self, loss): # -> None:
        """Computes the sum of gradients of the given tensor w.r.t. graph leaves.

        Compared to :func:`fairseq.optim.FairseqOptimizer.backward`, this
        function additionally dynamically scales the loss to avoid gradient
        underflow.
        """
        ...
    
    def step(self): # -> None:
        ...
    
    def clip_grad_norm(self, max_norm, aggregate_norm_fn=...):
        """Clips gradient norm."""
        ...
    
    @property
    def scaler(self): # -> GradScaler:
        ...
    
    @property
    def next_loss_scale(self): # -> float:
        ...
    
    @property
    def optimizer(self):
        ...
    
    @optimizer.setter
    def optimizer(self, optimizer): # -> None:
        ...
    
    @property
    def lr_scheduler(self): # -> Any | None:
        ...
    
    @property
    def optimizer_config(self):
        ...
    
    def get_lr(self):
        ...
    
    def set_lr(self, lr): # -> None:
        ...
    
    def all_reduce_grads(self, module): # -> None:
        ...
    
    @property
    def supports_flat_params(self):
        ...
    


