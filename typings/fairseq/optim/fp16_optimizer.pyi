"""
This type stub file was generated by pyright.
"""

from omegaconf import DictConfig
from fairseq import optim

class _FP16OptimizerMixin:
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    @property
    def has_flat_params(self): # -> bool:
        ...
    
    @classmethod
    def build_fp32_params(cls, args, params, flatten=...): # -> dict[Any, Any] | list[Any]:
        ...
    
    def state_dict(self):
        """Return the optimizer's state dict."""
        ...
    
    def load_state_dict(self, state_dict, optimizer_overrides=...): # -> None:
        """Load an optimizer state dict.

        In general we should prefer the configuration of the existing optimizer
        instance (e.g., learning rate) over that found in the state_dict. This
        allows us to resume training from a checkpoint using a new set of
        optimizer args.
        """
        ...
    
    def backward(self, loss): # -> None:
        """Computes the sum of gradients of the given tensor w.r.t. graph leaves.

        Compared to :func:`fairseq.optim.FairseqOptimizer.backward`, this
        function additionally dynamically scales the loss to avoid gradient
        underflow.
        """
        ...
    
    def multiply_grads(self, c): # -> None:
        """Multiplies grads by a constant ``c``."""
        ...
    
    def clip_grad_norm(self, max_norm, aggregate_norm_fn=...):
        """Clips gradient norm and updates dynamic loss scaler."""
        ...
    
    def step(self, closure=..., groups=...): # -> None:
        """Performs a single optimization step."""
        ...
    
    def zero_grad(self): # -> None:
        """Clears the gradients of all optimized parameters."""
        ...
    


class FP16Optimizer(_FP16OptimizerMixin, optim.FairseqOptimizer):
    """
    Wrap an *optimizer* to support FP16 (mixed precision) training.
    """
    def __init__(self, cfg: DictConfig, params, fp32_optimizer, fp32_params, **kwargs) -> None:
        ...
    
    @classmethod
    def build_optimizer(cls, cfg: DictConfig, params, **kwargs): # -> Self:
        """
        Args:
            cfg (omegaconf.DictConfig): fairseq args
            params (iterable): iterable of parameters to optimize
        """
        ...
    
    @property
    def optimizer(self):
        ...
    
    @optimizer.setter
    def optimizer(self, optimizer): # -> None:
        ...
    
    @property
    def lr_scheduler(self): # -> Any | None:
        ...
    
    @property
    def optimizer_config(self):
        ...
    
    def get_lr(self):
        ...
    
    def set_lr(self, lr): # -> None:
        ...
    
    def all_reduce_grads(self, module): # -> None:
        ...
    
    @property
    def supports_flat_params(self):
        ...
    


class _MemoryEfficientFP16OptimizerMixin:
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    @property
    def has_flat_params(self): # -> Literal[False]:
        ...
    
    def state_dict(self):
        """Return the optimizer's state dict."""
        ...
    
    def load_state_dict(self, state_dict, optimizer_overrides=...): # -> None:
        """Load an optimizer state dict.

        In general we should prefer the configuration of the existing optimizer
        instance (e.g., learning rate) over that found in the state_dict. This
        allows us to resume training from a checkpoint using a new set of
        optimizer args.
        """
        ...
    
    def backward(self, loss): # -> None:
        """Computes the sum of gradients of the given tensor w.r.t. graph leaves.

        Compared to :func:`fairseq.optim.FairseqOptimizer.backward`, this
        function additionally dynamically scales the loss to avoid gradient
        underflow.
        """
        ...
    
    def multiply_grads(self, c): # -> None:
        """Multiplies grads by a constant *c*."""
        ...
    
    def clip_grad_norm(self, max_norm, aggregate_norm_fn=...):
        """Clips gradient norm and updates dynamic loss scaler."""
        ...
    
    def step(self, closure=..., groups=...): # -> None:
        """Performs a single optimization step."""
        ...
    
    def zero_grad(self): # -> None:
        """Clears the gradients of all optimized parameters."""
        ...
    
    @property
    def supports_flat_params(self):
        ...
    


class MemoryEfficientFP16Optimizer(_MemoryEfficientFP16OptimizerMixin, optim.FairseqOptimizer):
    """
    Wrap an *optimizer* to support FP16 (mixed precision) training.

    Compared to :class:`fairseq.optim.FP16Optimizer`, this version does not
    maintain an FP32 copy of the model. We instead expect the optimizer to
    convert the gradients to FP32 internally and sync the results back to the
    FP16 model params. This significantly reduces memory usage but slightly
    increases the time spent in the optimizer.

    Since this wrapper depends on specific functionality in the wrapped
    optimizer (i.e., on-the-fly conversion of grads to FP32), only certain
    optimizers can be wrapped. This is determined by the
    *supports_memory_efficient_fp16* property.
    """
    def __init__(self, cfg: DictConfig, params, optimizer, allow_unsupported=..., **kwargs) -> None:
        ...
    
    @classmethod
    def build_optimizer(cls, cfg: DictConfig, params, **kwargs): # -> Self:
        """
        Args:
            args (argparse.Namespace): fairseq args
            params (iterable): iterable of parameters to optimize
        """
        ...
    
    @property
    def optimizer(self):
        ...
    
    @optimizer.setter
    def optimizer(self, optimizer): # -> None:
        ...
    
    @property
    def optimizer_config(self):
        ...
    
    @property
    def lr_scheduler(self): # -> Any | None:
        ...
    
    def get_lr(self):
        ...
    
    def set_lr(self, lr): # -> None:
        ...
    
    def all_reduce_grads(self, module): # -> None:
        ...
    


