"""
This type stub file was generated by pyright.
"""

class FairseqOptimizer:
    def __init__(self, cfg) -> None:
        ...
    
    @classmethod
    def add_args(cls, parser): # -> None:
        """Add optimizer-specific arguments to the parser."""
        ...
    
    @property
    def optimizer(self): # -> Optimizer:
        """Return a torch.optim.optimizer.Optimizer instance."""
        ...
    
    @optimizer.setter
    def optimizer(self, optimizer): # -> None:
        """Reset optimizer instance."""
        ...
    
    @property
    def optimizer_config(self):
        """
        Return a kwarg dictionary that will be used to override optimizer
        args stored in checkpoints. This allows us to load a checkpoint and
        resume training using a different set of optimizer args, e.g., with a
        different learning rate.
        """
        ...
    
    @property
    def params(self): # -> Generator[Any, Any, None]:
        """Return an iterable of the parameters held by the optimizer."""
        ...
    
    @property
    def param_groups(self): # -> List[Dict[str, Any]]:
        ...
    
    def __getstate__(self):
        ...
    
    def get_lr(self): # -> Any:
        """Return the current learning rate."""
        ...
    
    def set_lr(self, lr): # -> None:
        """Set the learning rate."""
        ...
    
    def state_dict(self): # -> Dict[str, Any]:
        """Return the optimizer's state dict."""
        ...
    
    def load_state_dict(self, state_dict, optimizer_overrides=...): # -> None:
        """Load an optimizer state dict.

        In general we should prefer the configuration of the existing optimizer
        instance (e.g., learning rate) over that found in the state_dict. This
        allows us to resume training from a checkpoint using a new set of
        optimizer args.
        """
        ...
    
    def backward(self, loss): # -> None:
        """Computes the sum of gradients of the given tensor w.r.t. graph leaves."""
        ...
    
    def all_reduce_grads(self, module): # -> None:
        """Manually all-reduce gradients (if required)."""
        ...
    
    def multiply_grads(self, c): # -> None:
        """Multiplies grads by a constant *c*."""
        ...
    
    def clip_grad_norm(self, max_norm, aggregate_norm_fn=...): # -> Tensor:
        """Clips gradient norm."""
        ...
    
    def step(self, closure=..., scale=..., groups=...): # -> None:
        """Performs a single optimization step."""
        ...
    
    def zero_grad(self): # -> None:
        """Clears the gradients of all optimized parameters."""
        ...
    
    @property
    def supports_memory_efficient_fp16(self): # -> Literal[False]:
        ...
    
    @property
    def supports_step_with_scale(self): # -> Literal[False]:
        ...
    
    @property
    def supports_groups(self): # -> Literal[False]:
        ...
    
    @property
    def supports_flat_params(self): # -> Literal[False]:
        """
        Whether the optimizer supports collapsing of the model
        parameters/gradients into a single contiguous Tensor.
        """
        ...
    
    def average_params(self): # -> None:
        ...
    
    def broadcast_global_state_dict(self, state_dict):
        """
        Broadcasts a global state dict to all ranks.
        Useful for optimizers that shard state between ranks.
        """
        ...
    


class LegacyFairseqOptimizer(FairseqOptimizer):
    def __init__(self, args) -> None:
        ...
    


