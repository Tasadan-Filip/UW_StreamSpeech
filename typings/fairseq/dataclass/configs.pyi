"""
This type stub file was generated by pyright.
"""

from dataclasses import dataclass
from typing import Any, List, Optional
from fairseq.dataclass.constants import DATASET_IMPL_CHOICES, DDP_BACKEND_CHOICES, DDP_COMM_HOOK_CHOICES, GENERATION_CONSTRAINTS_CHOICES, GENERATION_DECODING_FORMAT_CHOICES, LOG_FORMAT_CHOICES, PIPELINE_CHECKPOINT_CHOICES, PRINT_ALIGNMENT_CHOICES, ZERO_SHARDING_CHOICES

@dataclass
class FairseqDataclass:
    """fairseq base dataclass that supported fetching attributes and metas"""
    _name: Optional[str] = ...
    @staticmethod
    def name(): # -> None:
        ...
    
    @classmethod
    def from_namespace(cls, args): # -> Self:
        ...
    


@dataclass
class CommonConfig(FairseqDataclass):
    no_progress_bar: bool = ...
    log_interval: int = ...
    log_format: Optional[LOG_FORMAT_CHOICES] = ...
    log_file: Optional[str] = ...
    aim_repo: Optional[str] = ...
    aim_run_hash: Optional[str] = ...
    tensorboard_logdir: Optional[str] = ...
    wandb_project: Optional[str] = ...
    azureml_logging: Optional[bool] = ...
    seed: int = ...
    cpu: bool = ...
    tpu: bool = ...
    bf16: bool = ...
    memory_efficient_bf16: bool = ...
    fp16: bool = ...
    memory_efficient_fp16: bool = ...
    fp16_no_flatten_grads: bool = ...
    fp16_init_scale: int = ...
    fp16_scale_window: Optional[int] = ...
    fp16_scale_tolerance: float = ...
    on_cpu_convert_precision: bool = ...
    min_loss_scale: float = ...
    threshold_loss_scale: Optional[float] = ...
    amp: bool = ...
    amp_batch_retries: int = ...
    amp_init_scale: int = ...
    amp_scale_window: Optional[int] = ...
    user_dir: Optional[str] = ...
    empty_cache_freq: int = ...
    all_gather_list_size: int = ...
    model_parallel_size: int = ...
    quantization_config_path: Optional[str] = ...
    profile: bool = ...
    reset_logging: bool = ...
    suppress_crashes: bool = ...
    use_plasma_view: bool = ...
    plasma_path: Optional[str] = ...


@dataclass
class DistributedTrainingConfig(FairseqDataclass):
    distributed_world_size: int = ...
    distributed_num_procs: Optional[int] = ...
    distributed_rank: Optional[int] = ...
    distributed_backend: str = ...
    distributed_init_method: Optional[str] = ...
    distributed_port: int = ...
    device_id: int = ...
    distributed_no_spawn: bool = ...
    ddp_backend: DDP_BACKEND_CHOICES = ...
    ddp_comm_hook: DDP_COMM_HOOK_CHOICES = ...
    bucket_cap_mb: int = ...
    fix_batches_to_gpus: bool = ...
    find_unused_parameters: bool = ...
    gradient_as_bucket_view: bool = ...
    fast_stat_sync: bool = ...
    heartbeat_timeout: int = ...
    broadcast_buffers: bool = ...
    slowmo_momentum: Optional[float] = ...
    slowmo_base_algorithm: str = ...
    localsgd_frequency: int = ...
    nprocs_per_node: int = ...
    pipeline_model_parallel: bool = ...
    pipeline_balance: Optional[str] = ...
    pipeline_devices: Optional[str] = ...
    pipeline_chunks: Optional[int] = ...
    pipeline_encoder_balance: Optional[str] = ...
    pipeline_encoder_devices: Optional[str] = ...
    pipeline_decoder_balance: Optional[str] = ...
    pipeline_decoder_devices: Optional[str] = ...
    pipeline_checkpoint: PIPELINE_CHECKPOINT_CHOICES = ...
    zero_sharding: ZERO_SHARDING_CHOICES = ...
    fp16: bool = ...
    memory_efficient_fp16: bool = ...
    tpu: bool = ...
    no_reshard_after_forward: bool = ...
    fp32_reduce_scatter: bool = ...
    cpu_offload: bool = ...
    use_sharded_state: bool = ...
    not_fsdp_flatten_parameters: bool = ...


@dataclass
class DatasetConfig(FairseqDataclass):
    num_workers: int = ...
    skip_invalid_size_inputs_valid_test: bool = ...
    max_tokens: Optional[int] = ...
    batch_size: Optional[int] = ...
    required_batch_size_multiple: int = ...
    required_seq_len_multiple: int = ...
    dataset_impl: Optional[DATASET_IMPL_CHOICES] = ...
    data_buffer_size: int = ...
    train_subset: str = ...
    valid_subset: str = ...
    combine_valid_subsets: Optional[bool] = ...
    ignore_unused_valid_subsets: Optional[bool] = ...
    validate_interval: int = ...
    validate_interval_updates: int = ...
    validate_after_updates: int = ...
    fixed_validation_seed: Optional[int] = ...
    disable_validation: bool = ...
    max_tokens_valid: Optional[int] = ...
    batch_size_valid: Optional[int] = ...
    max_valid_steps: Optional[int] = ...
    curriculum: int = ...
    gen_subset: str = ...
    num_shards: int = ...
    shard_id: int = ...
    grouped_shuffling: bool = ...
    update_epoch_batch_itr: bool = ...
    update_ordered_indices_seed: bool = ...


@dataclass
class OptimizationConfig(FairseqDataclass):
    max_epoch: int = ...
    max_update: int = ...
    stop_time_hours: float = ...
    clip_norm: float = ...
    sentence_avg: bool = ...
    update_freq: List[int] = ...
    lr: List[float] = ...
    stop_min_lr: float = ...
    use_bmuf: bool = ...
    skip_remainder_batch: Optional[bool] = ...


@dataclass
class CheckpointConfig(FairseqDataclass):
    save_dir: str = ...
    restore_file: str = ...
    continue_once: Optional[str] = ...
    finetune_from_model: Optional[str] = ...
    reset_dataloader: bool = ...
    reset_lr_scheduler: bool = ...
    reset_meters: bool = ...
    reset_optimizer: bool = ...
    optimizer_overrides: str = ...
    save_interval: int = ...
    save_interval_updates: int = ...
    keep_interval_updates: int = ...
    keep_interval_updates_pattern: int = ...
    keep_last_epochs: int = ...
    keep_best_checkpoints: int = ...
    no_save: bool = ...
    no_epoch_checkpoints: bool = ...
    no_last_checkpoints: bool = ...
    no_save_optimizer_state: bool = ...
    best_checkpoint_metric: str = ...
    maximize_best_checkpoint_metric: bool = ...
    patience: int = ...
    checkpoint_suffix: str = ...
    checkpoint_shard_count: int = ...
    load_checkpoint_on_all_dp_ranks: bool = ...
    write_checkpoints_asynchronously: bool = ...
    model_parallel_size: int = ...


@dataclass
class FairseqBMUFConfig(FairseqDataclass):
    block_lr: float = ...
    block_momentum: float = ...
    global_sync_iter: int = ...
    warmup_iterations: int = ...
    use_nbm: bool = ...
    average_sync: bool = ...
    distributed_world_size: int = ...


@dataclass
class GenerationConfig(FairseqDataclass):
    beam: int = ...
    nbest: int = ...
    max_len_a: float = ...
    max_len_b: int = ...
    min_len: int = ...
    match_source_len: bool = ...
    unnormalized: bool = ...
    no_early_stop: bool = ...
    no_beamable_mm: bool = ...
    lenpen: float = ...
    unkpen: float = ...
    replace_unk: Optional[str] = ...
    sacrebleu: bool = ...
    score_reference: bool = ...
    prefix_size: int = ...
    no_repeat_ngram_size: int = ...
    sampling: bool = ...
    sampling_topk: int = ...
    sampling_topp: float = ...
    constraints: Optional[GENERATION_CONSTRAINTS_CHOICES] = ...
    temperature: float = ...
    diverse_beam_groups: int = ...
    diverse_beam_strength: float = ...
    diversity_rate: float = ...
    print_alignment: Optional[PRINT_ALIGNMENT_CHOICES] = ...
    print_step: bool = ...
    lm_path: Optional[str] = ...
    lm_weight: float = ...
    iter_decode_eos_penalty: float = ...
    iter_decode_max_iter: int = ...
    iter_decode_force_max_iter: bool = ...
    iter_decode_with_beam: int = ...
    iter_decode_with_external_reranker: bool = ...
    retain_iter_history: bool = ...
    retain_dropout: bool = ...
    retain_dropout_modules: Any = ...
    decoding_format: Optional[GENERATION_DECODING_FORMAT_CHOICES] = ...
    no_seed_provided: bool = ...
    eos_token: Optional[str] = ...


@dataclass
class CommonEvalConfig(FairseqDataclass):
    path: Optional[str] = ...
    post_process: Optional[str] = ...
    quiet: bool = ...
    model_overrides: str = ...
    results_path: Optional[str] = ...


@dataclass
class EvalLMConfig(FairseqDataclass):
    output_word_probs: bool = ...
    output_word_stats: bool = ...
    context_window: int = ...
    softmax_batch: int = ...


@dataclass
class InteractiveConfig(FairseqDataclass):
    buffer_size: int = ...
    input: str = ...


@dataclass
class EMAConfig(FairseqDataclass):
    store_ema: bool = ...
    ema_decay: float = ...
    ema_start_update: int = ...
    ema_seed_model: Optional[str] = ...
    ema_update_freq: int = ...
    ema_fp32: bool = ...


@dataclass
class FairseqConfig(FairseqDataclass):
    common: CommonConfig = ...
    common_eval: CommonEvalConfig = ...
    distributed_training: DistributedTrainingConfig = ...
    dataset: DatasetConfig = ...
    optimization: OptimizationConfig = ...
    checkpoint: CheckpointConfig = ...
    bmuf: FairseqBMUFConfig = ...
    generation: GenerationConfig = ...
    eval_lm: EvalLMConfig = ...
    interactive: InteractiveConfig = ...
    model: Any = ...
    task: Any = ...
    criterion: Any = ...
    optimizer: Any = ...
    lr_scheduler: Any = ...
    scoring: Any = ...
    bpe: Any = ...
    tokenizer: Any = ...
    ema: EMAConfig = ...


