"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from typing import List, Optional
from fairseq.token_generation_constraints import ConstraintState
from torch import Tensor

class Search(nn.Module):
    def __init__(self, tgt_dict) -> None:
        ...
    
    def step(self, step, lprobs, scores, prev_output_tokens=..., original_batch_idxs=...):
        """Take a single search step.

        Args:
            step: the current search step, starting at 0
            lprobs: (bsz x input_beam_size x vocab_size)
                the model's log-probabilities over the vocabulary at the current step
            scores: (bsz x input_beam_size x step)
                the historical model scores of each hypothesis up to this point
            prev_output_tokens: (bsz x step)
                the previously generated oputput tokens
            original_batch_idxs: (bsz)
                the tensor with the batch indices, in the range [0, bsz)
                this is useful in case there has been applied a re-ordering
                and we need to know the orignal indices

        Return: A tuple of (scores, indices, beams) where:
            scores: (bsz x output_beam_size)
                the scores of the chosen elements; output_beam_size can be
                larger than input_beam_size, e.g., we may return
                2*input_beam_size to account for EOS
            indices: (bsz x output_beam_size)
                the indices of the chosen elements
            beams: (bsz x output_beam_size)
                the hypothesis ids of the chosen elements, in the range [0, input_beam_size)
        """
        ...
    
    @torch.jit.export
    def set_src_lengths(self, src_lengths): # -> None:
        ...
    
    @torch.jit.export
    def init_constraints(self, batch_constraints: Optional[Tensor], beam_size: int): # -> None:
        """Initialize constraint states for constrained decoding (if supported).

        Args:
            batch_constraints: (torch.Tensor, optional)
                the list of constraints, in packed form
            beam_size: (int)
                the beam size
        Returns:
            *encoder_out* rearranged according to *new_order*
        """
        ...
    
    def prune_sentences(self, batch_idxs: Tensor): # -> None:
        """
        Removes constraint states for completed sentences (if supported).
        This is called from sequence_generator._generate() when sentences are
        deleted from the batch.

        Args:
            batch_idxs: Indices of *sentences* whose constraint state should be *kept*.
        """
        ...
    
    def update_constraints(self, active_hypos: Tensor): # -> None:
        """
        Updates the constraint states by selecting the beam items that are retained.
        This is called at each time step of sequence_generator._generate() when
        the set of 2 * {beam_size} candidate hypotheses are reduced to the beam size.

        Args:
            active_hypos: (batch size, beam size)
              list of integers denoting, for each sentence, which beam candidate items
              should be kept.
        """
        ...
    


class BeamSearch(Search):
    def __init__(self, tgt_dict) -> None:
        ...
    
    @torch.jit.export
    def step(self, step: int, lprobs, scores: Optional[Tensor], prev_output_tokens: Optional[Tensor] = ..., original_batch_idxs: Optional[Tensor] = ...): # -> tuple[Any, Any, Tensor]:
        ...
    


class PrefixConstrainedBeamSearch(Search):
    def __init__(self, tgt_dict, prefix_allowed_tokens_fn) -> None:
        ...
    
    @torch.jit.export
    def apply_mask(self, x, prev_output_tokens, original_batch_idxs): # -> Tensor:
        ...
    
    @torch.jit.export
    def step(self, step: int, lprobs: Tensor, scores: Tensor, prev_output_tokens: Tensor, original_batch_idxs: Tensor): # -> tuple[Any, Any, Any]:
        ...
    


class LexicallyConstrainedBeamSearch(Search):
    """Implements lexically constrained beam search as described in

        Fast Lexically Constrained Decoding with Dynamic Beam
        Allocation for Neural Machine Translation.  Post & Vilar,
        NAACL 2018.  https://www.aclweb.org/anthology/N18-1119/

    and

        Improved Lexically Constrained Decoding for Translation and
        Monolingual Rewriting. Hu et al, NAACL
        2019. https://www.aclweb.org/anthology/N19-1090/

    This is accomplished by maintaining, for each beam hypothesis, a
    ConstraintState object (see constraints.py) that tracks which
    constraints have been generated and using this information to
    shape the beam for each input sentence.
    """
    def __init__(self, tgt_dict, representation) -> None:
        ...
    
    @torch.jit.export
    def init_constraints(self, batch_constraints: Optional[Tensor], beam_size: int): # -> None:
        ...
    
    @torch.jit.export
    def prune_sentences(self, batch_idxs: Tensor): # -> None:
        ...
    
    @torch.jit.export
    def update_constraints(self, active_hypos: Tensor): # -> None:
        ...
    
    @torch.jit.export
    def step(self, step: int, lprobs: Tensor, scores: Optional[Tensor], prev_output_tokens: Optional[Tensor] = ..., original_batch_idxs: Optional[Tensor] = ...): # -> tuple[Any, Any, Any] | tuple[Tensor, Tensor, Tensor]:
        """
        A constrained step builds a large candidates list from the following:
        - the top 2 * {beam_size} items over the whole beam
        - for each item in the beam
          - the top {each_k} (default 1)
          - all next constraints
        We then compute the constrained state of each beam item, and assign
        stripe codes: 0 to the best in each bank, 1 to the 2nd-best, and so
        on. We then sort by (stripe, score), and truncate the list at
        2 * beam size.

        Args:
            step: the decoder step
            lprobs: (batch size, beam size, target vocab)
                the target-vocab distributions for each item in the beam.
        Retrun: A tuple of (scores, indices, beams, constraints) where:
            scores: (batch, output beam size)
                the scores of the chosen elements
            indices: (batch, output beam size)
                the target vocab indices of the chosen elements
            beams: (batch, output beam size)
                the 0-indexed hypothesis ids of the chosen elements
            constraints: (batch, output beam size)
                the new constraint states
        """
        ...
    
    @torch.jit.export
    def step_sentence(self, step: int, sentno: int, lprobs: Tensor, constraint_states: List[List[ConstraintState]], beams_buf: Tensor, indices_buf: Tensor, scores_buf: Tensor): # -> tuple[Tensor, Tensor, Tensor, list[List[ConstraintState]]]:
        """Does per-sentence processing. Adds all constraints for each
        hypothesis to the list of candidates; then removes duplicates,
        sorts, and dynamically stripes across the banks. All tensor inputs
        are collapsed to those pertaining to a single input sentence.
        """
        ...
    


class LengthConstrainedBeamSearch(Search):
    def __init__(self, tgt_dict, min_len_a, min_len_b, max_len_a, max_len_b) -> None:
        ...
    
    def step(self, step: int, lprobs, scores, prev_output_tokens: Optional[Tensor] = ..., original_batch_idxs: Optional[Tensor] = ...): # -> tuple[Any, Any, Tensor]:
        ...
    


class DiverseBeamSearch(Search):
    """Diverse Beam Search.

    See "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence
    Models" for details.

    We only implement the Hamming Diversity penalty here, which performed best
    in the original paper.
    """
    def __init__(self, tgt_dict, num_groups, diversity_strength) -> None:
        ...
    
    @torch.jit.export
    def step(self, step: int, lprobs, scores, prev_output_tokens: Optional[Tensor] = ..., original_batch_idxs: Optional[Tensor] = ...): # -> tuple[Tensor, Tensor, Tensor]:
        ...
    


class Sampling(Search):
    sampling_topk: int
    sampling_topp: float
    def __init__(self, tgt_dict, sampling_topk=..., sampling_topp=...) -> None:
        ...
    
    @torch.jit.export
    def step(self, step: int, lprobs, scores, prev_output_tokens: Optional[Tensor] = ..., original_batch_idxs: Optional[Tensor] = ...): # -> tuple[Tensor, Tensor, Tensor]:
        ...
    


class DiverseSiblingsSearch(Search):
    """
    Beam search with diverse siblings.

    See "A Simple, Fast Diverse Decoding Algorithm for Neural Generation" for details.
    https://arxiv.org/abs/1611.08562

    1/ Calculate hypotheses for each beam
    2/ Intra-sibling ordering
    3/ Rewrite scores
    4/ Choose top K hypotheses

    if diversity_rate == 0 is equivalent to BeamSearch
    """
    def __init__(self, tgt_dict, diversity_rate) -> None:
        ...
    
    def step(self, step: int, lprobs, scores, prev_output_tokens: Optional[Tensor] = ..., original_batch_idxs: Optional[Tensor] = ...): # -> tuple[Any, Any, Tensor] | tuple[Any, Any, Any]:
        ...
    


